---
# GitLab CI/CD Pipeline for Ajasta App

variables:
  # Docker Registry Configuration
  DOCKER_REGISTRY: $CI_REGISTRY
  DOCKER_IMAGE_TAG: $CI_COMMIT_SHA
  DOCKER_LATEST_TAG: "latest"

  # Application Configuration
  # Convert CI_PROJECT_PATH to lowercase for Docker registry compatibility
  # Note: Lowercase conversion is done in job scripts for shell compatibility
  BACKEND_IMAGE_NAME: "$CI_PROJECT_PATH/backend"
  FRONTEND_IMAGE_NAME: "$CI_PROJECT_PATH/frontend"

  # Yandex Cloud Configuration
  YC_ZONE: "ru-central1-a"
  YC_INSTANCE_NAME: "ajasta-app-vm"
  YC_INSTANCE_TYPE: "standard-v3"
  YC_CORES: "2"
  YC_MEMORY: "2GB"
  YC_DISK_SIZE: "10GB"

  # Build optimization for idempotency
  DOCKER_BUILDKIT: "1"
  BUILDX_EXPERIMENTAL: "1"

stages:
  - validate
  - build
  - test
  - package
  - deploy

# Template for Docker operations with retry mechanisms and timeout handling
.docker_base: &docker_base
  image: docker:24-dind
  services:
    - docker:24-dind
  variables:
    # Docker daemon configuration for better network handling
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: "/certs"
    # Registry connection timeouts and retries
    DOCKER_CLI_EXPERIMENTAL: enabled
  before_script:
    # Install coreutils for timeout command - compatible with both Alpine
    # (Docker) and macOS (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache coreutils
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - install coreutils if timeout is not available
        if ! command -v timeout >/dev/null 2>&1; then
          echo "Installing coreutils for timeout command on macOS..."
          brew install coreutils
        else
          echo "timeout command already available on macOS"
        fi
      else
        echo "Neither apk nor brew found. Assuming timeout command" \
             "is available."
      fi
    # Configure Docker daemon with retry and timeout settings
    - |
      mkdir -p ~/.docker
      cat > ~/.docker/config.json << EOF
      {
        "experimental": "enabled",
        "registry-mirrors": [],
        "max-concurrent-downloads": 3,
        "max-concurrent-uploads": 3
      }
      EOF
    # Retry mechanism for GitLab Container Registry login with backoff
    - |
      retry_count=0
      max_retries=5
      base_wait=2
      while [ $retry_count -lt $max_retries ]; do
        echo "Attempting GitLab registry login (attempt" \
             "$((retry_count + 1))/$max_retries)..."
        if echo $CI_REGISTRY_PASSWORD | timeout 30 docker login \
           -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY; then
          echo "‚úÖ GitLab registry login successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            wait_time=$((base_wait * retry_count))
            echo "‚ö†Ô∏è GitLab registry login failed, retrying in ${wait_time}s..."
            sleep $wait_time
          else
            echo "‚ùå GitLab registry login failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
    # Login to Docker Hub to avoid rate limiting (optional credentials)
    - |
      if [ -n "${DOCKER_HUB_USER:-}" ] && \
         [ -n "${DOCKER_HUB_PASSWORD:-}" ]; then
        echo "Attempting Docker Hub login..."
        retry_count=0
        max_retries=3
        while [ $retry_count -lt $max_retries ]; do
          if echo $DOCKER_HUB_PASSWORD | timeout 30 docker login \
             -u $DOCKER_HUB_USER --password-stdin; then
            echo "‚úÖ Docker Hub login successful"
            break
          else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
              echo "‚ö†Ô∏è Docker Hub login failed, retrying" \
                   "($((retry_count + 1))/$max_retries)..."
              sleep 5
            else
              echo "‚ö†Ô∏è Docker Hub login failed after $max_retries attempts," \
                   "proceeding without authentication"
              break
            fi
          fi
        done
      else
        echo "‚ÑπÔ∏è Docker Hub credentials not provided, proceeding without" \
             "authentication (may hit rate limits)"
      fi
    - timeout 10 docker info || (echo "Docker info timeout" && exit 1)

# Validate stage - check code quality and configuration
validate:syntax:
  stage: validate
  image: alpine:latest
  before_script:
    # Install packages - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache yamllint shellcheck
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor)
        brew install yamllint shellcheck
      else
        echo "Neither apk nor brew found. Please ensure required packages" \
             "are installed."
        exit 1
      fi
  script:
    - yamllint .gitlab-ci.yml
    - yamllint docker-compose.yml
    - find scripts -name "*.sh" -o -name "*.zsh" | xargs shellcheck || true
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate:docker:
  <<: *docker_base
  stage: validate
  script:
    - docker run --rm -i hadolint/hadolint < ajasta-backend/Dockerfile
    - docker run --rm -i hadolint/hadolint < ajasta-react/Dockerfile
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Helm lint - validate Helm charts
helm:lint:
  stage: validate
  image: alpine/helm:latest
  before_script:
    # Install Helm - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if ! command -v helm >/dev/null 2>&1; then
        echo "Helm not found, installing..."
        if command -v apk >/dev/null 2>&1; then
          # Alpine Linux (Docker executor) - helm already in alpine/helm image
          echo "Running in Alpine/Docker environment"
        elif command -v brew >/dev/null 2>&1; then
          # macOS (shell executor) - install helm via brew
          echo "Installing Helm on macOS..."
          brew install helm
        else
          echo "Neither apk nor brew found. Please install Helm manually."
          exit 1
        fi
      else
        echo "‚úÖ Helm is already installed"
        helm version
      fi
  script:
    - echo "üîç Linting Helm charts..."
    - helm lint helm/ajasta-app
    - echo "‚úÖ Helm chart validation successful"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Build stage - compile and prepare artifacts
build:backend:
  <<: *docker_base
  stage: build
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    # Check if image with this commit SHA already exists (idempotency)
    # - with retry
    - |
      echo "Checking if backend image already exists..."
      retry_count=0
      max_retries=3
      image_exists=false
      while [ $retry_count -lt $max_retries ]; do
        if timeout 60 docker manifest inspect \
           $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           > /dev/null 2>&1; then
          echo "‚úÖ Backend image already exists for commit $DOCKER_IMAGE_TAG," \
               "skipping build"
          image_exists=true
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Manifest inspect failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 5
          fi
        fi
      done
      if [ "$image_exists" = true ]; then
        exit 0
      fi
      echo "Image does not exist or manifest check failed," \
           "proceeding with build..."
    # Build with cache optimization and retry mechanisms
    - cd ajasta-backend
    - |
      echo "Building backend Docker image..."
      retry_count=0
      max_retries=3
      while [ $retry_count -lt $max_retries ]; do
        if timeout 600 docker build \
           --cache-from \
           $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           --tag $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           --tag $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           . ; then
          echo "‚úÖ Backend Docker build successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Docker build failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 10
          else
            echo "‚ùå Docker build failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
    # Push images with retry mechanism
    - |
      for tag in "$DOCKER_IMAGE_TAG" "$DOCKER_LATEST_TAG"; do
        echo "Pushing $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$tag..."
        retry_count=0
        max_retries=5
        while [ $retry_count -lt $max_retries ]; do
          if timeout 300 docker push \
             $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$tag; then
            echo "‚úÖ Successfully pushed" \
                 "$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$tag"
            break
          else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
              wait_time=$((5 * retry_count))
              echo "‚ö†Ô∏è Push failed, retrying in ${wait_time}s" \
                   "($((retry_count + 1))/$max_retries)..."
              sleep $wait_time
            else
              echo "‚ùå Failed to push" \
                   "$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$tag" \
                   "after $max_retries attempts"
              exit 1
            fi
          fi
        done
      done
  artifacts:
    reports:
      dotenv: build.env
  only:
    - main
    - develop
    - tags

build:frontend:
  <<: *docker_base
  stage: build
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Check if image with this commit SHA already exists (idempotency)
    # - with retry
    - |
      echo "Checking if frontend image already exists..."
      retry_count=0
      max_retries=3
      image_exists=false
      while [ $retry_count -lt $max_retries ]; do
        if timeout 60 docker manifest inspect \
           $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           > /dev/null 2>&1; then
          echo "‚úÖ Frontend image already exists for commit $DOCKER_IMAGE_TAG," \
               "skipping build"
          image_exists=true
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Manifest inspect failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 5
          fi
        fi
      done
      if [ "$image_exists" = true ]; then
        exit 0
      fi
      echo "Image does not exist or manifest check failed," \
           "proceeding with build..."
    # Build with configurable API URL for different environments
    - cd ajasta-react
    - |
      if [ "$CI_COMMIT_REF_NAME" = "main" ]; then
        API_BASE_URL="http://$YC_VM_EXTERNAL_IP:8090/api"
      else
        API_BASE_URL="http://localhost:8090/api"
      fi
    - |
      echo "Building frontend Docker image..."
      retry_count=0
      max_retries=3
      while [ $retry_count -lt $max_retries ]; do
        if timeout 600 docker build \
           --cache-from \
           $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           --build-arg API_BASE_URL="$API_BASE_URL" \
           --tag $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           --tag $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           . ; then
          echo "‚úÖ Frontend Docker build successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Docker build failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 10
          else
            echo "‚ùå Docker build failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
    # Push images with retry mechanism
    - |
      for tag in "$DOCKER_IMAGE_TAG" "$DOCKER_LATEST_TAG"; do
        echo "Pushing $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$tag..."
        retry_count=0
        max_retries=5
        while [ $retry_count -lt $max_retries ]; do
          if timeout 300 docker push \
             $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$tag; then
            echo "‚úÖ Successfully pushed" \
                 "$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$tag"
            break
          else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
              wait_time=$((5 * retry_count))
              echo "‚ö†Ô∏è Push failed, retrying in ${wait_time}s" \
                   "($((retry_count + 1))/$max_retries)..."
              sleep $wait_time
            else
              echo "‚ùå Failed to push" \
                   "$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$tag" \
                   "after $max_retries attempts"
              exit 1
            fi
          fi
        done
      done
  only:
    - main
    - develop
    - tags

# Test stage - run automated tests
test:backend:
  stage: test
  image: maven:3.9.8-eclipse-temurin-21-alpine
  services:
    - postgres:16-alpine
  variables:
    MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
    # PostgreSQL service configuration
    POSTGRES_DB: testdb
    POSTGRES_USER: testuser
    POSTGRES_PASSWORD: testpass
    # Spring Boot test configuration
    SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/testdb
    SPRING_DATASOURCE_USERNAME: testuser
    SPRING_DATASOURCE_PASSWORD: testpass
  cache:
    key: "$CI_JOB_NAME"
    paths:
      - .m2/repository/
      - ajasta-backend/target/
  script:
    - cd ajasta-backend
    - mvn clean verify
  artifacts:
    reports:
      junit:
        - ajasta-backend/target/surefire-reports/TEST-*.xml
        - ajasta-backend/target/failsafe-reports/TEST-*.xml
    paths:
      - ajasta-backend/target/
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

test:frontend:
  stage: test
  image: node:20-alpine
  cache:
    key: "$CI_JOB_NAME"
    paths:
      - ajasta-react/node_modules/
  script:
    - cd ajasta-react
    - npm ci --no-audit --no-fund
    - npm run test -- --coverage --watchAll=false
    - npm run build
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: ajasta-react/coverage/cobertura-coverage.xml
    paths:
      - ajasta-react/coverage/
      - ajasta-react/build/
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Package stage - create deployment artifacts
package:compose:
  stage: package
  image: alpine:latest
  script:
    # Install envsubst - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache gettext
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - envsubst is part of gettext package
        brew install gettext
      else
        echo "Neither apk nor brew found. Please ensure gettext package" \
             "is installed."
        exit 1
      fi
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Create deployment package with environment-specific configuration
    - mkdir -p deploy
    # Substitute image tags in docker-compose for deployment
    - >
      envsubst '${DOCKER_REGISTRY} ${BACKEND_IMAGE_LOWER}
      ${FRONTEND_IMAGE_LOWER} ${DOCKER_IMAGE_TAG}'
      < docker-compose.yml > deploy/docker-compose.yml
    - cp -r scripts deploy/
    - echo "DOCKER_IMAGE_TAG=$DOCKER_IMAGE_TAG" > deploy/.env
    - BACKEND_IMG="$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG"
    - echo "BACKEND_IMAGE=$BACKEND_IMG" >> deploy/.env
    - FRONTEND_IMG="$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG"
    - echo "FRONTEND_IMAGE=$FRONTEND_IMG" >> deploy/.env
  artifacts:
    paths:
      - deploy/
    expire_in: 1 week
  only:
    - main
    - develop
    - tags

# Deploy stage - deploy to Yandex Cloud
.deploy_base: &deploy_base
  stage: deploy
  image: alpine:latest
  before_script:
    # Install dependencies - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache curl bash openssh-client
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - curl and openssh are usually pre-installed,
        # bash might need update
        brew install curl openssh
        # bash is pre-installed on macOS, but we can ensure it's updated
        # if needed
        brew install bash || true
      else
        echo "Neither apk nor brew found. Please ensure curl, bash, and" \
             "openssh are installed."
        exit 1
      fi
    # Install Yandex Cloud CLI
    - curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
    - export PATH=$PATH:/root/yandex-cloud/bin
    # Authenticate with Yandex Cloud
    - yc config set token $YC_TOKEN
    - yc config set cloud-id $YC_CLOUD_ID
    - yc config set folder-id $YC_FOLDER_ID
    # Setup SSH
    - eval $(ssh-agent -s)
    - echo "$YC_SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan -H $YC_VM_EXTERNAL_IP >> ~/.ssh/known_hosts 2>/dev/null \
        || true

deploy:staging:
  <<: *deploy_base
  environment:
    name: staging
    url: http://$YC_VM_EXTERNAL_IP
  script:
    - cd deploy/scripts && ./deploy-yc.zsh --mode vm
  only:
    - develop
  when: manual

deploy:production:
  <<: *deploy_base
  environment:
    name: production
    url: http://$YC_VM_EXTERNAL_IP
  script:
    - cd deploy/scripts && ./deploy-yc.zsh --mode vm
  only:
    - main
    - tags
  when: manual

# Kubernetes Deployment with Helm
# Base template for Kubernetes deployments
.k8s_deploy_base: &k8s_deploy_base
  stage: deploy
  image: alpine/helm:latest
  before_script:
    # Install dependencies - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache curl kubectl
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor)
        # curl is pre-installed on macOS, install kubectl if needed
        if ! command -v kubectl >/dev/null 2>&1; then
          echo "Installing kubectl on macOS..."
          brew install kubectl
        else
          echo "‚úÖ kubectl is already installed"
        fi
      else
        echo "Neither apk nor brew found. Please ensure curl and kubectl" \
             "are installed."
        exit 1
      fi
    # Ensure Helm is available
    - |
      if ! command -v helm >/dev/null 2>&1; then
        echo "Helm not found, installing..."
        if command -v apk >/dev/null 2>&1; then
          # Alpine Linux (Docker executor) - helm should be in alpine/helm image
          echo "Running in Alpine/Docker environment - Helm should be present"
          exit 1
        elif command -v brew >/dev/null 2>&1; then
          # macOS (shell executor) - install helm via brew
          echo "Installing Helm on macOS..."
          brew install helm
        else
          echo "Cannot install Helm. Please install it manually."
          exit 1
        fi
      else
        echo "‚úÖ Helm is already installed"
        helm version
      fi
    # Setup kubeconfig from GitLab CI/CD variable
    - mkdir -p ~/.kube
    - echo "$KUBECONFIG_CONTENT" | base64 -d > ~/.kube/config
    - chmod 600 ~/.kube/config
    # Configure cluster to skip TLS verification (required for public IP access)
    # The API server certificate is issued for internal IPs, but CI/CD accesses
    # via public IP, causing certificate validation to fail
    - kubectl config set-cluster kubernetes --insecure-skip-tls-verify=true
    - echo "‚úÖ Configured cluster to skip TLS verification"
    # Verify cluster connectivity
    - kubectl cluster-info
    - kubectl version --client
    - helm version

deploy:k8s:staging:
  <<: *k8s_deploy_base
  environment:
    name: k8s-staging
    url: http://$K8S_STAGING_INGRESS_HOST
  variables:
    KUBE_NAMESPACE: ajasta-staging
    RELEASE_NAME: ajasta-staging
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    - echo "üöÄ Deploying Ajasta App to Kubernetes Staging..."
    # Set ingress host variable
    - INGRESS_HOST=${K8S_STAGING_INGRESS_HOST:-ajasta-staging.local}
    # Deploy or upgrade with Helm
    - |
      helm upgrade --install $RELEASE_NAME ./helm/ajasta-app \
        --create-namespace \
        --namespace $KUBE_NAMESPACE \
        --set global.namespace=$KUBE_NAMESPACE \
        --set backend.image.repository=$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER \
        --set backend.image.tag=$DOCKER_IMAGE_TAG \
        --set frontend.image.repository=$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER \
        --set frontend.image.tag=$DOCKER_IMAGE_TAG \
        --set ingress.hosts[0].host=$INGRESS_HOST \
        --set postgres.auth.password=$POSTGRES_PASSWORD \
        --set backend.database.password=$POSTGRES_PASSWORD \
        --set backend.secrets.jwtSecret=$JWT_SECRET \
        --wait \
        --timeout 10m
    - echo "‚úÖ Deployment to Kubernetes Staging successful"
    # Display deployment status
    - kubectl get all -n $KUBE_NAMESPACE
  only:
    - develop
  when: manual

deploy:k8s:production:
  <<: *k8s_deploy_base
  environment:
    name: k8s-production
    url: http://$K8S_PRODUCTION_INGRESS_HOST
  variables:
    KUBE_NAMESPACE: ajasta
    RELEASE_NAME: ajasta-production
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    - echo "üöÄ Deploying Ajasta App to Kubernetes Production..."
    # Set ingress host variable
    - INGRESS_HOST=${K8S_PRODUCTION_INGRESS_HOST:-ajasta.local}
    # Deploy or upgrade with Helm
    - |
      helm upgrade --install $RELEASE_NAME ./helm/ajasta-app \
        --create-namespace \
        --namespace $KUBE_NAMESPACE \
        --set global.namespace=$KUBE_NAMESPACE \
        --set backend.image.repository=$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER \
        --set backend.image.tag=$DOCKER_IMAGE_TAG \
        --set frontend.image.repository=$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER \
        --set frontend.image.tag=$DOCKER_IMAGE_TAG \
        --set ingress.hosts[0].host=$INGRESS_HOST \
        --set postgres.auth.password=$POSTGRES_PASSWORD \
        --set backend.database.password=$POSTGRES_PASSWORD \
        --set backend.secrets.jwtSecret=$JWT_SECRET \
        --set backend.secrets.awsAccessKeyId=$AWS_ACCESS_KEY_ID \
        --set backend.secrets.awsSecretAccessKey=$AWS_SECRET_ACCESS_KEY \
        --set backend.config.awsS3Bucket=$AWS_S3_BUCKET \
        --set backend.secrets.stripePublicKey=$STRIPE_PUBLIC_KEY \
        --set backend.secrets.stripeSecretKey=$STRIPE_SECRET_KEY \
        --wait \
        --timeout 10m
    - echo "‚úÖ Deployment to Kubernetes Production successful"
    # Display deployment status
    - kubectl get all -n $KUBE_NAMESPACE
  only:
    - main
    - tags
  when: manual

# Cleanup old images for storage optimization (idempotency maintenance)
cleanup:registry:
  <<: *docker_base
  stage: deploy
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Keep only latest 10 images for each service
    - |
      for image in $BACKEND_IMAGE_LOWER $FRONTEND_IMAGE_LOWER; do
        REPO_URL="$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories"
        TAGS=$(curl -s -H "Authorization: Bearer $CI_JOB_TOKEN" \
          "$REPO_URL/$image/tags" | jq -r '.[].name' | head -n -10)
        for tag in $TAGS; do
          if [ "$tag" != "$DOCKER_LATEST_TAG" ] && \
             [ "$tag" != "$DOCKER_IMAGE_TAG" ]; then
            curl -X DELETE -H "Authorization: Bearer $CI_JOB_TOKEN" \
              "$REPO_URL/$image/tags/$tag" || true
          fi
        done
      done
  only:
    - schedules
  when: manual
