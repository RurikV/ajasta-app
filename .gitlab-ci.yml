---
# GitLab CI/CD Pipeline for Ajasta App

variables:
  # Docker Registry Configuration
  DOCKER_REGISTRY: $CI_REGISTRY
  DOCKER_IMAGE_TAG: $CI_COMMIT_SHA
  DOCKER_LATEST_TAG: "latest"

  # Application Configuration
  # Convert CI_PROJECT_PATH to lowercase for Docker registry compatibility
  # Note: Lowercase conversion is done in job scripts for shell compatibility
  BACKEND_IMAGE_NAME: "$CI_PROJECT_PATH/backend"
  FRONTEND_IMAGE_NAME: "$CI_PROJECT_PATH/frontend"

  # Yandex Cloud Configuration
  YC_ZONE: "ru-central1-a"
  YC_INSTANCE_NAME: "ajasta-app-vm"
  YC_INSTANCE_TYPE: "standard-v3"
  YC_CORES: "2"
  YC_MEMORY: "2GB"
  YC_DISK_SIZE: "10GB"

  # Build optimization for idempotency
  DOCKER_BUILDKIT: "1"
  BUILDX_EXPERIMENTAL: "1"

stages:
  - validate
  - build
  - test
  - package
  - deploy

# Template for Docker operations with retry mechanisms and timeout handling
.docker_base: &docker_base
  image: docker:24-dind
  services:
    - docker:24-dind
  variables:
    # Docker daemon configuration for better network handling
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: "/certs"
    # Registry connection timeouts and retries
    DOCKER_CLI_EXPERIMENTAL: enabled
  before_script:
    # Install coreutils for timeout command - compatible with both Alpine
    # (Docker) and macOS (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache coreutils
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - install coreutils if timeout is not available
        if ! command -v timeout >/dev/null 2>&1; then
          echo "Installing coreutils for timeout command on macOS..."
          brew install coreutils
        else
          echo "timeout command already available on macOS"
        fi
      else
        echo "Neither apk nor brew found. Assuming timeout command" \
             "is available."
      fi
    # Configure Docker daemon with retry and timeout settings
    - |
      mkdir -p ~/.docker
      cat > ~/.docker/config.json << EOF
      {
        "experimental": "enabled",
        "registry-mirrors": [],
        "max-concurrent-downloads": 3,
        "max-concurrent-uploads": 3
      }
      EOF
    # Retry mechanism for GitLab Container Registry login with backoff
    - |
      retry_count=0
      max_retries=5
      base_wait=2
      while [ $retry_count -lt $max_retries ]; do
        echo "Attempting GitLab registry login (attempt" \
             "$((retry_count + 1))/$max_retries)..."
        if echo $CI_REGISTRY_PASSWORD | timeout 30 docker login \
           -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY; then
          echo "‚úÖ GitLab registry login successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            wait_time=$((base_wait * retry_count))
            echo "‚ö†Ô∏è GitLab registry login failed, retrying in ${wait_time}s..."
            sleep $wait_time
          else
            echo "‚ùå GitLab registry login failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
    # Login to Docker Hub to avoid rate limiting (optional credentials)
    - |
      if [ -n "${DOCKER_HUB_USER:-}" ] && \
         [ -n "${DOCKER_HUB_PASSWORD:-}" ]; then
        echo "Attempting Docker Hub login..."
        retry_count=0
        max_retries=3
        while [ $retry_count -lt $max_retries ]; do
          if echo $DOCKER_HUB_PASSWORD | timeout 30 docker login \
             -u $DOCKER_HUB_USER --password-stdin; then
            echo "‚úÖ Docker Hub login successful"
            break
          else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
              echo "‚ö†Ô∏è Docker Hub login failed, retrying" \
                   "($((retry_count + 1))/$max_retries)..."
              sleep 5
            else
              echo "‚ö†Ô∏è Docker Hub login failed after $max_retries attempts," \
                   "proceeding without authentication"
              break
            fi
          fi
        done
      else
        echo "‚ÑπÔ∏è Docker Hub credentials not provided, proceeding without" \
             "authentication (may hit rate limits)"
      fi
    - timeout 10 docker info || (echo "Docker info timeout" && exit 1)
    # Setup buildx for multi-architecture builds (ARM64 + AMD64)
    - |
      docker buildx create --name multiarch-builder --use || \
        docker buildx use multiarch-builder || true
    - docker buildx inspect --bootstrap

# Validate stage - check code quality and configuration
validate:syntax:
  stage: validate
  image: alpine:latest
  before_script:
    # Install packages - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache yamllint shellcheck
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor)
        brew install yamllint shellcheck
      else
        echo "Neither apk nor brew found. Please ensure required packages" \
             "are installed."
        exit 1
      fi
  script:
    - yamllint .gitlab-ci.yml
    - yamllint docker-compose.yml
    - find scripts -name "*.sh" -o -name "*.zsh" | xargs shellcheck || true
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate:docker:
  <<: *docker_base
  stage: validate
  script:
    - docker run --rm -i hadolint/hadolint < ajasta-backend/Dockerfile
    - docker run --rm -i hadolint/hadolint < ajasta-react/Dockerfile
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Helm lint - validate Helm charts
helm:lint:
  stage: validate
  image: alpine/helm:latest
  before_script:
    # Install Helm - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if ! command -v helm >/dev/null 2>&1; then
        echo "Helm not found, installing..."
        if command -v apk >/dev/null 2>&1; then
          # Alpine Linux (Docker executor) - helm already in alpine/helm image
          echo "Running in Alpine/Docker environment"
        elif command -v brew >/dev/null 2>&1; then
          # macOS (shell executor) - install helm via brew
          echo "Installing Helm on macOS..."
          brew install helm
        else
          echo "Neither apk nor brew found. Please install Helm manually."
          exit 1
        fi
      else
        echo "‚úÖ Helm is already installed"
        helm version
      fi
  script:
    - echo "üîç Linting Helm charts..."
    - helm lint helm/ajasta-app
    - echo "‚úÖ Helm chart validation successful"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Kubernetes auth smoke test to catch stale kubeconfig early
k8s:auth:check:
  stage: validate
  image: alpine/helm:latest
  before_script:
    - |
      if command -v apk >/dev/null 2>&1; then
        apk add --no-cache curl kubectl
      elif command -v brew >/dev/null 2>&1; then
        if ! command -v kubectl >/dev/null 2>&1; then
          brew install kubectl
        fi
      else
        echo "Neither apk nor brew found. Please install kubectl manually."
        exit 1
      fi
  script:
    - mkdir -p .kube
    - echo "$KUBECONFIG_CONTENT" | base64 -d > .kube/config
    - chmod 600 .kube/config
    - export KUBECONFIG="$CI_PROJECT_DIR/.kube/config"
    - |
      CLUSTER_NAME="$(kubectl config view \
        -o jsonpath='{.clusters[0].name}' 2>/dev/null || echo kubernetes)"
      kubectl config unset "clusters.${CLUSTER_NAME}.certificate-authority" \
        >/dev/null 2>&1 || true
      kubectl config unset \
        "clusters.${CLUSTER_NAME}.certificate-authority-data" \
        >/dev/null 2>&1 || true
      kubectl config set-cluster "$CLUSTER_NAME" \
        --insecure-skip-tls-verify=true >/dev/null
      CTX_NAME="$(kubectl config view \
        -o jsonpath='{.contexts[0].name}' 2>/dev/null || \
        echo kubernetes-admin@kubernetes)"
      if kubectl config get-contexts "$CTX_NAME" >/dev/null 2>&1; then
        kubectl config use-context "$CTX_NAME"
      else
        kubectl config use-context "$(kubectl config get-contexts -o name \
          | head -n1)"
      fi
    - kubectl cluster-info
    - kubectl get nodes -o wide
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
  allow_failure: false

# Build stage - compile and prepare artifacts
build:backend:
  <<: *docker_base
  stage: build
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    # Check if image with this commit SHA already exists (idempotency)
    # - with retry
    - |
      echo "Checking if backend image already exists..."
      retry_count=0
      max_retries=3
      image_exists=false
      while [ $retry_count -lt $max_retries ]; do
        if timeout 60 docker manifest inspect \
           $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           > /dev/null 2>&1; then
          echo "‚úÖ Backend image already exists for commit $DOCKER_IMAGE_TAG," \
               "skipping build"
          image_exists=true
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Manifest inspect failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 5
          fi
        fi
      done
      if [ "$image_exists" = true ]; then
        exit 0
      fi
      echo "Image does not exist or manifest check failed," \
           "proceeding with build..."
    # Build with cache optimization and multi-architecture support
    - cd ajasta-backend
    - |
      echo "Building multi-arch backend Docker image" \
           "(linux/amd64,linux/arm64)..."
      retry_count=0
      max_retries=3
      CACHE_REF="$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_LATEST_TAG"
      while [ $retry_count -lt $max_retries ]; do
        if timeout 900 docker buildx build \
           --platform linux/amd64,linux/arm64 \
           --cache-from type=registry,ref=$CACHE_REF \
           --tag $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           --tag $DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           --push \
           . ; then
          echo "‚úÖ Backend multi-arch Docker build and push successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Docker buildx failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 10
          else
            echo "‚ùå Docker buildx failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
  artifacts:
    reports:
      dotenv: build.env
  only:
    - main
    - develop
    - tags

build:frontend:
  <<: *docker_base
  stage: build
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Check if image with this commit SHA already exists (idempotency)
    # - with retry
    - |
      echo "Checking if frontend image already exists..."
      retry_count=0
      max_retries=3
      image_exists=false
      while [ $retry_count -lt $max_retries ]; do
        if timeout 60 docker manifest inspect \
           $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           > /dev/null 2>&1; then
          echo "‚úÖ Frontend image already exists for commit $DOCKER_IMAGE_TAG," \
               "skipping build"
          image_exists=true
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Manifest inspect failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 5
          fi
        fi
      done
      if [ "$image_exists" = true ]; then
        exit 0
      fi
      echo "Image does not exist or manifest check failed," \
           "proceeding with build..."
    # Build with configurable API URL for different environments
    - cd ajasta-react
    - |
      if [ "$CI_COMMIT_REF_NAME" = "main" ]; then
        API_BASE_URL="http://$YC_VM_EXTERNAL_IP:8090/api"
      else
        API_BASE_URL="http://localhost:8090/api"
      fi
    - |
      echo "Building multi-arch frontend Docker image" \
           "(linux/amd64,linux/arm64)..."
      retry_count=0
      max_retries=3
      CACHE_REF="$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_LATEST_TAG"
      while [ $retry_count -lt $max_retries ]; do
        if timeout 900 docker buildx build \
           --platform linux/amd64,linux/arm64 \
           --cache-from type=registry,ref=$CACHE_REF \
           --build-arg API_BASE_URL="$API_BASE_URL" \
           --tag $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG \
           --tag $DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_LATEST_TAG \
           --push \
           . ; then
          echo "‚úÖ Frontend multi-arch Docker build and push successful"
          break
        else
          retry_count=$((retry_count + 1))
          if [ $retry_count -lt $max_retries ]; then
            echo "‚ö†Ô∏è Docker buildx failed, retrying" \
                 "($((retry_count + 1))/$max_retries)..."
            sleep 10
          else
            echo "‚ùå Docker buildx failed after $max_retries attempts"
            exit 1
          fi
        fi
      done
  only:
    - main
    - develop
    - tags

# Test stage - run automated tests
test:backend:
  stage: test
  image: maven:3.9.8-eclipse-temurin-21-alpine
  services:
    - postgres:16-alpine
  variables:
    MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
    # PostgreSQL service configuration
    POSTGRES_DB: testdb
    POSTGRES_USER: testuser
    POSTGRES_PASSWORD: testpass
    # Spring Boot test configuration
    SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/testdb
    SPRING_DATASOURCE_USERNAME: testuser
    SPRING_DATASOURCE_PASSWORD: testpass
  cache:
    key: "$CI_JOB_NAME"
    paths:
      - .m2/repository/
      - ajasta-backend/target/
  script:
    - cd ajasta-backend
    - mvn clean verify
  artifacts:
    reports:
      junit:
        - ajasta-backend/target/surefire-reports/TEST-*.xml
        - ajasta-backend/target/failsafe-reports/TEST-*.xml
    paths:
      - ajasta-backend/target/
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

test:frontend:
  stage: test
  image: node:20-alpine
  cache:
    key: "$CI_JOB_NAME"
    paths:
      - ajasta-react/node_modules/
  script:
    - cd ajasta-react
    - npm ci --no-audit --no-fund
    - npm run test -- --coverage --watchAll=false
    - npm run build
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: ajasta-react/coverage/cobertura-coverage.xml
    paths:
      - ajasta-react/coverage/
      - ajasta-react/build/
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# Package stage - create deployment artifacts
package:compose:
  stage: package
  image: alpine:latest
  script:
    # Install envsubst - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache gettext
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - envsubst is part of gettext package
        brew install gettext
      else
        echo "Neither apk nor brew found. Please ensure gettext package" \
             "is installed."
        exit 1
      fi
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Create deployment package with environment-specific configuration
    - mkdir -p deploy
    # Substitute image tags in docker-compose for deployment
    - >
      envsubst '${DOCKER_REGISTRY} ${BACKEND_IMAGE_LOWER}
      ${FRONTEND_IMAGE_LOWER} ${DOCKER_IMAGE_TAG}'
      < docker-compose.yml > deploy/docker-compose.yml
    - cp -r scripts deploy/
    - echo "DOCKER_IMAGE_TAG=$DOCKER_IMAGE_TAG" > deploy/.env
    - BACKEND_IMG="$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG"
    - echo "BACKEND_IMAGE=$BACKEND_IMG" >> deploy/.env
    - FRONTEND_IMG="$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER:$DOCKER_IMAGE_TAG"
    - echo "FRONTEND_IMAGE=$FRONTEND_IMG" >> deploy/.env
  artifacts:
    paths:
      - deploy/
    expire_in: 1 week
  only:
    - main
    - develop
    - tags

# Deploy stage - deploy to Yandex Cloud
.deploy_base: &deploy_base
  stage: deploy
  image: alpine:latest
  before_script:
    # Install dependencies - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache curl bash openssh-client
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor) - curl and openssh are usually pre-installed,
        # bash might need update
        brew install curl openssh
        # bash is pre-installed on macOS, but we can ensure it's updated
        # if needed
        brew install bash || true
      else
        echo "Neither apk nor brew found. Please ensure curl, bash, and" \
             "openssh are installed."
        exit 1
      fi
    # Install Yandex Cloud CLI
    - curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
    - export PATH=$PATH:/root/yandex-cloud/bin
    # Authenticate with Yandex Cloud
    - yc config set token $YC_TOKEN
    - yc config set cloud-id $YC_CLOUD_ID
    - yc config set folder-id $YC_FOLDER_ID
    # Setup SSH
    - eval $(ssh-agent -s)
    - echo "$YC_SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan -H $YC_VM_EXTERNAL_IP >> ~/.ssh/known_hosts 2>/dev/null \
        || true

deploy:staging:
  <<: *deploy_base
  environment:
    name: staging
    url: http://$YC_VM_EXTERNAL_IP
  script:
    - cd deploy/scripts && ./deploy-yc.zsh --mode vm
  only:
    - develop
  when: manual

deploy:production:
  <<: *deploy_base
  environment:
    name: production
    url: http://$YC_VM_EXTERNAL_IP
  script:
    - cd deploy/scripts && ./deploy-yc.zsh --mode vm
  only:
    - main
    - tags
  when: manual

# Kubernetes Deployment with Helm
# Base template for Kubernetes deployments
.k8s_deploy_base: &k8s_deploy_base
  stage: deploy
  image: alpine/helm:latest
  before_script:
    # Install dependencies - compatible with both Alpine (Docker) and macOS
    # (shell executor)
    - |
      if command -v apk >/dev/null 2>&1; then
        # Alpine Linux (Docker executor)
        apk add --no-cache curl kubectl
      elif command -v brew >/dev/null 2>&1; then
        # macOS (shell executor)
        # curl is pre-installed on macOS, install kubectl if needed
        if ! command -v kubectl >/dev/null 2>&1; then
          echo "Installing kubectl on macOS..."
          brew install kubectl
        else
          echo "‚úÖ kubectl is already installed"
        fi
      else
        echo "Neither apk nor brew found. Please ensure curl and kubectl" \
             "are installed."
        exit 1
      fi
    # Ensure Helm is available
    - |
      if ! command -v helm >/dev/null 2>&1; then
        echo "Helm not found, installing..."
        if command -v apk >/dev/null 2>&1; then
          # Alpine Linux (Docker executor) - helm should be in alpine/helm image
          echo "Running in Alpine/Docker environment - Helm should be present"
          exit 1
        elif command -v brew >/dev/null 2>&1; then
          # macOS (shell executor) - install helm via brew
          echo "Installing Helm on macOS..."
          brew install helm
        else
          echo "Cannot install Helm. Please install it manually."
          exit 1
        fi
      else
        echo "‚úÖ Helm is already installed"
        helm version
      fi
    # Setup kubeconfig from GitLab CI/CD variable
    # (project-local, do not touch ~/.kube)
    - mkdir -p .kube
    - echo "$KUBECONFIG_CONTENT" | base64 -d > .kube/config
    - chmod 600 .kube/config
    - export KUBECONFIG="$CI_PROJECT_DIR/.kube/config"
    # Normalize cluster/context and TLS settings; avoid CA + insecure conflict
    - |
      CLUSTER_NAME="$(kubectl config view \
        -o jsonpath='{.clusters[0].name}' 2>/dev/null || echo kubernetes)"
      CTX_NAME="$(kubectl config view \
        -o jsonpath='{.contexts[0].name}' 2>/dev/null || \
        echo kubernetes-admin@kubernetes)"
      CA_KEY="clusters.${CLUSTER_NAME}.certificate-authority"
      CA_DATA_KEY="clusters.${CLUSTER_NAME}.certificate-authority-data"
      kubectl config unset "$CA_KEY" >/dev/null 2>&1 || true
      kubectl config unset "$CA_DATA_KEY" >/dev/null 2>&1 || true
      kubectl config set-cluster "$CLUSTER_NAME" \
        --insecure-skip-tls-verify=true >/dev/null
      if kubectl config get-contexts "$CTX_NAME" >/dev/null 2>&1; then
        kubectl config use-context "$CTX_NAME"
      else
        FIRST_CTX="$(kubectl config get-contexts -o name | head -n1)"
        kubectl config use-context "$FIRST_CTX"
      fi
    - echo "‚úÖ Configured cluster to skip TLS verification"
    # Verify cluster connectivity with diagnostics on failure
    - |
      if ! kubectl cluster-info; then
        echo "‚ö†Ô∏è kubectl auth diagnostics (minified config):"
        kubectl config view --minify || true
        echo "‚öôÔ∏è  /version:"
        kubectl get --raw=/version || true
        echo "üîê auth can-i:"
        kubectl auth can-i --list || true
        exit 1
      fi
    - kubectl version --client
    - helm version

deploy:k8s:staging:
  <<: *k8s_deploy_base
  environment:
    name: k8s-staging
    url: http://$K8S_STAGING_INGRESS_HOST
  variables:
    KUBE_NAMESPACE: ajasta-staging
    RELEASE_NAME: ajasta-staging
    K8S_CLEAN_NS: "true"
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    - echo "üöÄ Deploying Ajasta App to Kubernetes Staging..."
    # Set ingress host variable
    - INGRESS_HOST=${K8S_STAGING_INGRESS_HOST:-ajasta-staging.local}
    # Deploy or upgrade with Helm
    - |
      # Preflight: ensure namespace state for first install (optional cleanup)
      if ! helm status "$RELEASE_NAME" -n "$KUBE_NAMESPACE" \
        >/dev/null 2>&1; then
        echo "‚ÑπÔ∏è Helm release '$RELEASE_NAME' not found in namespace" \
             "'$KUBE_NAMESPACE' (fresh install)."
        if [ "${K8S_CLEAN_NS:-false}" = "true" ]; then
          if kubectl get namespace "$KUBE_NAMESPACE" >/dev/null 2>&1; then
            echo "üßπ Deleting existing namespace '$KUBE_NAMESPACE' before" \
                 "install..."
            kubectl delete namespace "$KUBE_NAMESPACE" --wait --timeout=5m
          else
            echo "‚úÖ Namespace '$KUBE_NAMESPACE' does not exist."
          fi
        else
          echo "‚ö†Ô∏è Skipping namespace cleanup (K8S_CLEAN_NS=false)."
        fi
      else
        echo "‚úÖ Helm release '$RELEASE_NAME' exists - upgrade path."
      fi

      # Patch ingress controller Service with current external IP
      # (resilient to IP rotation)
      if [ -n "${YC_VM_EXTERNAL_IP:-}" ]; then
        JSON_PATCH='{"spec":{"externalIPs":["'${YC_VM_EXTERNAL_IP}'"]}}'
        kubectl -n ingress-nginx patch svc \
          ingress-nginx-controller \
          --type merge \
          -p "$JSON_PATCH" || true
        kubectl -n ingress-nginx get svc \
          ingress-nginx-controller -o wide || true
      fi

      # Compute Helm host flag (use hostless catch-all if INGRESS_HOST is empty)
      if [ -n "$INGRESS_HOST" ]; then
        HELM_HOST_FLAG="--set ingress.hosts[0].host=$INGRESS_HOST"
      else
        HELM_HOST_FLAG="--set-string ingress.hosts[0].host=\"\""
      fi

      helm upgrade --install $RELEASE_NAME ./helm/ajasta-app \
        --create-namespace \
        --namespace $KUBE_NAMESPACE \
        --set namespace.create=false \
        --set global.namespace=$KUBE_NAMESPACE \
        --set backend.image.repository=$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER \
        --set backend.image.tag=$DOCKER_IMAGE_TAG \
        --set frontend.image.repository=$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER \
        --set frontend.image.tag=$DOCKER_IMAGE_TAG \
        $HELM_HOST_FLAG \
        --set ingress.hosts[0].paths[0].path=/ \
        --set ingress.hosts[0].paths[0].pathType=Prefix \
        --set ingress.hosts[0].paths[0].serviceName=ajasta-frontend \
        --set ingress.hosts[0].paths[0].servicePort=80 \
        --set ingress.hosts[0].paths[1].path=/api \
        --set ingress.hosts[0].paths[1].pathType=Prefix \
        --set ingress.hosts[0].paths[1].serviceName=ajasta-backend \
        --set ingress.hosts[0].paths[1].servicePort=8090 \
        --set postgres.auth.password=$POSTGRES_PASSWORD \
        --set backend.database.password=$POSTGRES_PASSWORD \
        --set backend.secrets.jwtSecret=$JWT_SECRET \
        --wait \
        --timeout 10m

      # Post-deploy quick checks
      kubectl -n $KUBE_NAMESPACE get ingress ajasta-ingress || true
      # Probe homepage
      for i in 1 2 3 4 5; do
        if curl -fsSI "http://$INGRESS_HOST/" >/dev/null 2>&1; then
          echo "‚úÖ Ingress homepage reachable"; break; fi; sleep 5; done || true
      # Probe public API endpoint
      for i in 1 2 3 4 5; do
        if curl -fsSI \
          "http://$INGRESS_HOST/api/resources?active=true" \
          >/dev/null 2>&1; then
          echo "‚úÖ Public API reachable"; break; fi; sleep 5; done || true
      # Basic diagnostics
      kubectl -n $KUBE_NAMESPACE describe ingress ajasta-ingress \
        | sed -n '1,120p' || true
      kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide \
        || true
    - echo "‚úÖ Deployment to Kubernetes Staging successful"
    # Display deployment status
    - kubectl get all -n $KUBE_NAMESPACE
  only:
    - develop
  when: manual

deploy:k8s:production:
  <<: *k8s_deploy_base
  environment:
    name: k8s-production
    url: http://$K8S_PRODUCTION_INGRESS_HOST
  variables:
    KUBE_NAMESPACE: ajasta
    RELEASE_NAME: ajasta-production
    K8S_CLEAN_NS: "true"
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    - echo "üöÄ Deploying Ajasta App to Kubernetes Production..."
    # Set ingress host variable
    - INGRESS_HOST=${K8S_PRODUCTION_INGRESS_HOST:-$YC_VM_EXTERNAL_IP}
    # Deploy or upgrade with Helm
    - |
      # Preflight: ensure namespace state for first install (optional cleanup)
      if ! helm status "$RELEASE_NAME" -n "$KUBE_NAMESPACE" \
        >/dev/null 2>&1; then
        echo "‚ÑπÔ∏è Fresh install: release missing in" \
             "'$KUBE_NAMESPACE'"
        if [ "${K8S_CLEAN_NS:-false}" = "true" ]; then
          if kubectl get namespace "$KUBE_NAMESPACE" >/dev/null 2>&1; then
            echo "üßπ Deleting namespace '$KUBE_NAMESPACE'..."
            kubectl delete namespace "$KUBE_NAMESPACE" --wait --timeout=5m
          else
            echo "‚úÖ Namespace '$KUBE_NAMESPACE' absent"
          fi
        else
          echo "‚ö†Ô∏è Skip ns cleanup (K8S_CLEAN_NS=false)"
        fi
      else
        echo "‚úÖ Release exists: upgrade"
      fi

      # Patch ingress controller Service with current external IP
      # (resilient to IP rotation)
      if [ -n "${YC_VM_EXTERNAL_IP:-}" ]; then
        JSON_PATCH='{"spec":{"externalIPs":["'${YC_VM_EXTERNAL_IP}'"]}}'
        kubectl -n ingress-nginx patch svc \
          ingress-nginx-controller \
          --type merge \
          -p "$JSON_PATCH" || true
        kubectl -n ingress-nginx get svc \
          ingress-nginx-controller -o wide || true
      fi

      # Compute Helm host flag (use hostless catch-all if INGRESS_HOST is empty)
      if [ -n "$INGRESS_HOST" ]; then
        HELM_HOST_FLAG="--set ingress.hosts[0].host=$INGRESS_HOST"
      else
        HELM_HOST_FLAG="--set-string ingress.hosts[0].host=\"\""
      fi

      helm upgrade --install $RELEASE_NAME ./helm/ajasta-app \
        --create-namespace \
        --namespace $KUBE_NAMESPACE \
        --set namespace.create=false \
        --set global.namespace=$KUBE_NAMESPACE \
        --set backend.image.repository=$DOCKER_REGISTRY/$BACKEND_IMAGE_LOWER \
        --set backend.image.tag=$DOCKER_IMAGE_TAG \
        --set frontend.image.repository=$DOCKER_REGISTRY/$FRONTEND_IMAGE_LOWER \
        --set frontend.image.tag=$DOCKER_IMAGE_TAG \
        $HELM_HOST_FLAG \
        --set ingress.hosts[0].paths[0].path=/ \
        --set ingress.hosts[0].paths[0].pathType=Prefix \
        --set ingress.hosts[0].paths[0].serviceName=ajasta-frontend \
        --set ingress.hosts[0].paths[0].servicePort=80 \
        --set ingress.hosts[0].paths[1].path=/api \
        --set ingress.hosts[0].paths[1].pathType=Prefix \
        --set ingress.hosts[0].paths[1].serviceName=ajasta-backend \
        --set ingress.hosts[0].paths[1].servicePort=8090 \
        --set postgres.auth.password=$POSTGRES_PASSWORD \
        --set backend.database.password=$POSTGRES_PASSWORD \
        --set backend.secrets.jwtSecret=$JWT_SECRET \
        --set backend.secrets.awsAccessKeyId=$AWS_ACCESS_KEY_ID \
        --set backend.secrets.awsSecretAccessKey=$AWS_SECRET_ACCESS_KEY \
        --set backend.config.awsS3Bucket=$AWS_S3_BUCKET \
        --set backend.secrets.stripePublicKey=$STRIPE_PUBLIC_KEY \
        --set backend.secrets.stripeSecretKey=$STRIPE_SECRET_KEY \
        --wait \
        --timeout 10m

      # Post-deploy quick checks
      kubectl -n $KUBE_NAMESPACE get ingress ajasta-ingress || true
      # Probe homepage
      for i in 1 2 3 4 5; do
        if curl -fsSI "http://$INGRESS_HOST/" >/dev/null 2>&1; then
          echo "‚úÖ Ingress homepage reachable"; break; fi; sleep 5; done || true
      # Probe public API endpoint
      for i in 1 2 3 4 5; do
        if curl -fsSI \
          "http://$INGRESS_HOST/api/resources?active=true" \
          >/dev/null 2>&1; then
          echo "‚úÖ Public API reachable"; break; fi; sleep 5; done || true
      # Basic diagnostics
      kubectl -n $KUBE_NAMESPACE describe ingress ajasta-ingress \
        | sed -n '1,120p' || true
      kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide \
        || true
    - echo "‚úÖ Deployment to Kubernetes Production successful"
    # Display deployment status
    - kubectl get all -n $KUBE_NAMESPACE
  only:
    - main
    - tags
  when: manual

# Cleanup old images for storage optimization (idempotency maintenance)
cleanup:registry:
  <<: *docker_base
  stage: deploy
  script:
    # Convert project path to lowercase for Docker registry compatibility
    - PROJECT_PATH_LOWER=$(echo "$CI_PROJECT_PATH" | tr '[:upper:]' '[:lower:]')
    - BACKEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/backend"
    - FRONTEND_IMAGE_LOWER="$PROJECT_PATH_LOWER/frontend"
    # Keep only latest 10 images for each service
    - |
      for image in $BACKEND_IMAGE_LOWER $FRONTEND_IMAGE_LOWER; do
        REPO_URL="$CI_API_V4_URL/projects/$CI_PROJECT_ID/registry/repositories"
        TAGS=$(curl -s -H "Authorization: Bearer $CI_JOB_TOKEN" \
          "$REPO_URL/$image/tags" | jq -r '.[].name' | head -n -10)
        for tag in $TAGS; do
          if [ "$tag" != "$DOCKER_LATEST_TAG" ] && \
             [ "$tag" != "$DOCKER_IMAGE_TAG" ]; then
            curl -X DELETE -H "Authorization: Bearer $CI_JOB_TOKEN" \
              "$REPO_URL/$image/tags/$tag" || true
          fi
        done
      done
  only:
    - schedules
  when: manual


# --- Deploy support jobs: patch ingress external IP and post-deploy probe ---
.deploy_kube_base: &deploy_kube_base
  image: bitnami/kubectl:1.28
  variables:
    KUBECONFIG: "$CI_PROJECT_DIR/.kube/config"
  before_script:
    - mkdir -p "$CI_PROJECT_DIR/.kube"
    - |
      if [ -n "${KUBECONFIG_CONTENT:-}" ]; then
        echo "$KUBECONFIG_CONTENT" | base64 -d > "$KUBECONFIG" 2>/dev/null || \
          echo "$KUBECONFIG_CONTENT" | base64 -D > "$KUBECONFIG"
      elif [ -f "$CI_PROJECT_DIR/kubeconfig" ]; then
        cp "$CI_PROJECT_DIR/kubeconfig" "$KUBECONFIG"
      else
        echo "KUBECONFIG_CONTENT not set and no kubeconfig file provided" >&2
        exit 1
      fi
    - kubectl version --short || true

# Patches ingress-nginx controller Service with the current public IP
# Use INGRESS_PUBLIC_IP (preferred) or YC_VM_EXTERNAL_IP as the source
# Example: INGRESS_PUBLIC_IP=51.250.100.218

deploy:patch-ingress-externalip:
  stage: deploy
  extends: [".deploy_kube_base"]
  rules:
    - if: >-
        $CI_PIPELINE_SOURCE == "push" &&
        $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: '"web" == $CI_PIPELINE_SOURCE'
    - if: '"pipeline" == $CI_PIPELINE_SOURCE'
  artifacts:
    reports:
      dotenv: deploy_env.env
    when: always
    expire_in: 1 day
  script:
    - |
      set -e
      IP="${INGRESS_PUBLIC_IP:-${YC_VM_EXTERNAL_IP:-}}"
      if [ -z "$IP" ]; then
        echo "INGRESS_PUBLIC_IP/YC_VM_EXTERNAL_IP not set." >&2
        echo "Trying to autodetect from ingress/service..." >&2
        # Try Ingress LoadBalancer IP/hostname
        JSONPATH_IP='{.status.loadBalancer.ingress[0].ip}'
        JSONPATH_HOST='{.status.loadBalancer.ingress[0].hostname}'
        IP=$(kubectl -n ajasta get ingress ajasta-ingress \
          -o jsonpath="$JSONPATH_IP" 2>/dev/null | tr -d '\n') || true
        if [ -z "$IP" ]; then
          IP=$(kubectl -n ajasta get ingress ajasta-ingress \
            -o jsonpath="$JSONPATH_HOST" 2>/dev/null | tr -d '\n') || true
        fi
        # Try existing Service externalIPs[0]
        if [ -z "$IP" ]; then
          IP=$(kubectl -n ingress-nginx get svc ingress-nginx-controller \
            -o jsonpath='{.spec.externalIPs[0]}' 2>/dev/null | tr -d '\n') \
            || true
        fi
      fi
      if [ -z "$IP" ]; then
        echo "ERROR: Could not determine public IP." >&2
        echo "Set INGRESS_PUBLIC_IP in CI variables." >&2
        exit 1
      fi
      echo "Patching ingress-nginx-controller with externalIPs=[$IP]"
      PATCH_PAYLOAD=$(cat <<'JSON'
      {"spec":{"externalIPs":["IP_PLACEHOLDER"]}}
      JSON
      )
      PATCH_PAYLOAD=${PATCH_PAYLOAD/IP_PLACEHOLDER/$IP}
      kubectl -n ingress-nginx patch svc ingress-nginx-controller \
        --type merge -p "$PATCH_PAYLOAD" || true
      echo "Service summary:"
      kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide || true
      echo "Ingress summary:"
      kubectl -n ajasta get ingress ajasta-ingress || true
      echo "Exporting PROBE_IP via dotenv artifact for downstream jobs"
      printf "PROBE_IP=%s\n" "$IP" > deploy_env.env

# Probes the root URL and /api after deploy to catch regressions

deploy:probe-root:
  stage: deploy
  needs: ["deploy:patch-ingress-externalip"]
  image: curlimages/curl:8.10.1
  variables:
    PROBE_IP: "$INGRESS_PUBLIC_IP"
  rules:
    - if: >-
        $CI_PIPELINE_SOURCE == "push" &&
        $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: '"web" == $CI_PIPELINE_SOURCE'
    - if: '"pipeline" == $CI_PIPELINE_SOURCE'
  script:
    - |
      set -e
      IP="${PROBE_IP:-${YC_VM_EXTERNAL_IP:-}}"
      if [ -z "$IP" ]; then
        echo "ERROR: Set INGRESS_PUBLIC_IP or YC_VM_EXTERNAL_IP for probe." >&2
        exit 1
      fi
      BASE="http://$IP"
      echo "Probing $BASE/"
      curl -sS -o /dev/null -w "HTTP %{http_code}\\n" "$BASE/"
      echo "Probing $BASE/api/health (fallback: /api)"
      code=$(curl -sS -o /dev/null -w "%{http_code}" \
        "$BASE/api/health" || true)
      if [ "$code" = "000" ] || [ "$code" = "404" ]; then
        curl -sS -o /dev/null -w "HTTP %{http_code}\\n" \
          "$BASE/api" || true
      else
        echo "HTTP $code"
      fi
