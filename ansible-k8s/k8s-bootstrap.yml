---
- name: Bootstrap Kubernetes on existing VMs (master + workers)
  hosts: local
  gather_facts: false
  vars:
    env_base:
      YC_CLOUD_ID: "{{ yc_cloud_id }}"
      YC_FOLDER_ID: "{{ yc_folder_id }}"
      YC_ZONE: "{{ yc_zone }}"
  tasks:
    - name: Populate YC IDs from environment if not set
      ansible.builtin.set_fact:
        yc_cloud_id: "{{ lookup('env','YC_CLOUD_ID') | default('', true) }}"
        yc_folder_id: "{{ lookup('env','YC_FOLDER_ID') | default('', true) }}"
      when:
        - (yc_cloud_id | default('') | length) == 0 or (yc_folder_id | default('') | length) == 0

    - name: Detect YC cloud-id from yc CLI if still not set
      ansible.builtin.command: yc config get cloud-id
      register: yc_cloud_id_cmd
      changed_when: false
      failed_when: false
      when: (yc_cloud_id | default('') | length) == 0

    - name: Set yc_cloud_id from yc CLI result
      ansible.builtin.set_fact:
        yc_cloud_id: "{{ yc_cloud_id_cmd.stdout | trim }}"
      when:
        - (yc_cloud_id | default('') | length) == 0
        - (yc_cloud_id_cmd.stdout | default('') | trim | length) > 0

    - name: Detect YC folder-id from yc CLI if still not set
      ansible.builtin.command: yc config get folder-id
      register: yc_folder_id_cmd
      changed_when: false
      failed_when: false
      when: (yc_folder_id | default('') | length) == 0

    - name: Set yc_folder_id from yc CLI result
      ansible.builtin.set_fact:
        yc_folder_id: "{{ yc_folder_id_cmd.stdout | trim }}"
      when:
        - (yc_folder_id | default('') | length) == 0
        - (yc_folder_id_cmd.stdout | default('') | trim | length) > 0

    - name: Validate required vars are set
      ansible.builtin.assert:
        that:
          - yc_cloud_id | length > 0
          - yc_folder_id | length > 0
        fail_msg: "yc_cloud_id and yc_folder_id must be provided in group_vars/all.yml or via YC_CLOUD_ID/YC_FOLDER_ID env vars or yc CLI config (yc init)"

    # Resolve IPs from existing instances
    - name: Resolve master public IP from YC
      ansible.builtin.shell: |
        yc compute instance get --name "{{ master_vm_name }}" --format json | \
        jq -r '.network_interfaces[0].primary_v4_address.one_to_one_nat.address // empty'
      args:
        chdir: "{{ playbook_dir }}/.."
      register: master_ip_out
      changed_when: false

    - name: Resolve worker1 internal IP from YC
      ansible.builtin.shell: |
        yc compute instance get --name "{{ worker1_vm_name }}" --format json | \
        jq -r '.network_interfaces[0].primary_v4_address.address // empty'
      args:
        chdir: "{{ playbook_dir }}/.."
      register: worker1_ip_out
      changed_when: false

    - name: Resolve worker2 internal IP from YC
      ansible.builtin.shell: |
        yc compute instance get --name "{{ worker2_vm_name }}" --format json | \
        jq -r '.network_interfaces[0].primary_v4_address.address // empty'
      args:
        chdir: "{{ playbook_dir }}/.."
      register: worker2_ip_out
      changed_when: false

    - name: Derive clean master and worker IPs
      ansible.builtin.set_fact:
        master_ip_clean: "{{ master_ip_out.stdout | trim }}"
        worker1_ip_clean: "{{ worker1_ip_out.stdout | trim }}"
        worker2_ip_clean: "{{ worker2_ip_out.stdout | trim }}"

    - name: Assert master IP resolved
      ansible.builtin.assert:
        that:
          - (master_ip_clean | length) > 0
        fail_msg: "Failed to resolve master public IP; ensure instances exist."

    - name: Prepare worker IP CSV
      ansible.builtin.set_fact:
        workers_csv: "{{ [worker1_ip_clean, worker2_ip_clean] | reject('equalto','') | join(',') }}"

    # Build dynamic inventory for other-k8s roles (group `k8s`)
    - name: Add master host to dynamic inventory (group k8s)
      ansible.builtin.add_host:
        name: k8s-master
        groups: k8s
        ansible_host: "{{ master_ip_clean }}"
        ansible_user: "{{ ssh_username }}"
        ansible_become: true
        ansible_become_method: sudo
        ansible_become_timeout: 60
        ansible_pipelining: true
        ansible_ssh_private_key_file: "{{ ssh_privkey_guess if (ssh_privkey_stat.stat.exists | default(false)) else omit }}"
        ansible_ssh_common_args: "-o BatchMode=yes -o NumberOfPasswordPrompts=0 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null -o CheckHostIP=no -o ConnectTimeout=10 -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ConnectionAttempts=10"

    - name: Add worker1 host to dynamic inventory (via ProxyJump through master)
      ansible.builtin.add_host:
        name: k8s-worker-1
        groups: k8s
        ansible_host: "{{ worker1_ip_clean }}"
        ansible_user: "{{ ssh_username }}"
        ansible_become: true
        ansible_become_method: sudo
        ansible_become_timeout: 60
        ansible_pipelining: true
        ansible_ssh_private_key_file: "{{ ssh_privkey_guess if (ssh_privkey_stat.stat.exists | default(false)) else omit }}"
        ansible_ssh_common_args: >-
          -o BatchMode=yes -o NumberOfPasswordPrompts=0 -o ProxyJump={{ ssh_username }}@{{ master_ip_clean }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null -o CheckHostIP=no -o ConnectTimeout=10 -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ConnectionAttempts=10
      when:
        - (worker1_ip_clean | default('') | length) > 0

    - name: Add worker2 host to dynamic inventory (via ProxyJump through master)
      ansible.builtin.add_host:
        name: k8s-worker-2
        groups: k8s
        ansible_host: "{{ worker2_ip_clean }}"
        ansible_user: "{{ ssh_username }}"
        ansible_become: true
        ansible_become_method: sudo
        ansible_become_timeout: 60
        ansible_pipelining: true
        ansible_ssh_private_key_file: "{{ ssh_privkey_guess if (ssh_privkey_stat.stat.exists | default(false)) else omit }}"
        ansible_ssh_common_args: >-
          -o BatchMode=yes -o NumberOfPasswordPrompts=0 -o ProxyJump={{ ssh_username }}@{{ master_ip_clean }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null -o CheckHostIP=no -o ConnectTimeout=10 -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ConnectionAttempts=10
      when:
        - (worker2_ip_clean | default('') | length) > 0

    - name: Precheck SSH reachability to master (fail fast)
      ansible.builtin.wait_for:
        host: "{{ master_ip_clean }}"
        port: 22
        timeout: 60
        sleep: 3
        state: started
      tags: ["bootstrap"]

    # Build optional SSH private key env (derive from public key path)
    - name: Determine HOME directory
      ansible.builtin.set_fact:
        home_dir: "{{ lookup('env','HOME') }}"

    - name: Resolve public key path (from group var)
      ansible.builtin.set_fact:
        ssh_pubkey_resolved: "{{ (ssh_pubkey_file | default('') | replace('~', home_dir)) }}"

    - name: Derive private key guess and check
      ansible.builtin.set_fact:
        ssh_privkey_guess: "{{ (ssh_pubkey_resolved | default('')) | regex_replace('\\.pub$', '') }}"

    - name: Check if derived SSH private key exists
      ansible.builtin.stat:
        path: "{{ ssh_privkey_guess }}"
      register: ssh_privkey_stat
      when: (ssh_privkey_guess | default('') | length) > 0

    - name: Build optional SSH private key env dict
      ansible.builtin.set_fact:
        ssh_privkey_env_optional: "{{ {'SSH_KEY_FILE': ssh_privkey_guess} if (ssh_privkey_stat.stat.exists | default(false)) else {} }}"

    - name: Bootstrap Kubernetes cluster (kubeadm)
      ansible.builtin.shell: |
        zsh {{ scripts_dir }}/setup-k8s-cluster.zsh "{{ master_ip_clean }}" "{{ workers_csv }}"
      args:
        chdir: "{{ playbook_dir }}/.."
      environment: "{{ env_base | combine({'SSH_USERNAME': ssh_username, 'SSH_RETRY_ATTEMPTS': ssh_retry_attempts | string, 'SSH_RETRY_DELAY': ssh_retry_delay | string, 'SSH_CMD_TIMEOUT': ssh_cmd_timeout | string}) | combine(ssh_privkey_env_optional) }}"
      async: 7200
      poll: 15
      register: k8s_bootstrap_out
      when:
        - not (use_other_k8s | default(true) | bool)
      tags: ["bootstrap"]

    - name: Show cluster bootstrap output (summary)
      ansible.builtin.debug:
        var: k8s_bootstrap_out.stdout_lines
      tags: ["bootstrap"]

    - name: Install Rancher dashboard on cluster (optional)
      ansible.builtin.shell: |
        zsh {{ scripts_dir }}/install-rancher.zsh "{{ master_ip_clean }}"
      args:
        chdir: "{{ playbook_dir }}/.."
      environment: "{{ env_base | combine({'SSH_USERNAME': ssh_username, 'RANCHER_HOSTNAME': rancher_hostname | default('rancher.local')}) | combine(ssh_privkey_env_optional) }}"
      register: rancher_install_out
      when: (install_rancher | default(true) | bool) and (not (use_other_k8s | default(true) | bool))
      tags: ["rancher"]

    - name: Show Rancher installation output
      ansible.builtin.debug:
        var: rancher_install_out.stdout_lines
      when: (install_rancher | default(true) | bool) and (not (use_other_k8s | default(true) | bool))
      tags: ["rancher"]


# Reuse the working approach from ./other-k8s (role-based bootstrap)
# Note: import_playbook is static; execution will no-op if group 'k8s' is empty
- import_playbook: ./other-k8s-playbook.yml


# Post-bootstrap Rancher installation when using other-k8s roles
- hosts: local
  gather_facts: false
  vars:
    env_base:
      YC_CLOUD_ID: "{{ yc_cloud_id }}"
      YC_FOLDER_ID: "{{ yc_folder_id }}"
      YC_ZONE: "{{ yc_zone }}"
  tasks:
    - name: Install Rancher dashboard on cluster (other-k8s path)
      ansible.builtin.shell: |
        zsh {{ scripts_dir }}/install-rancher.zsh "{{ master_ip_clean }}"
      args:
        chdir: "{{ playbook_dir }}/.."
      environment: "{{ env_base | combine({'SSH_USERNAME': ssh_username, 'RANCHER_HOSTNAME': rancher_hostname | default('rancher.local')}) | combine(ssh_privkey_env_optional) }}"
      register: rancher_install_other_out
      when: (install_rancher | default(true) | bool) and (use_other_k8s | default(true) | bool)
      tags: ["rancher"]

    - name: Show Rancher installation output (other-k8s path)
      ansible.builtin.debug:
        var: rancher_install_other_out.stdout_lines
      when: (install_rancher | default(true) | bool) and (use_other_k8s | default(true) | bool)
      tags: ["rancher"]
