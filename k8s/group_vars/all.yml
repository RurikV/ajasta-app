# Global variables for ansible-k8s playbooks

# Yandex Cloud context
yc_cloud_id: ""
yc_folder_id: ""
yc_zone: "ru-central1-b"

# Networking (external/public not used for workers)
yc_network_name: "external-ajasta-network"
yc_subnet_name: "ajasta-external-segment"
yc_subnet_cidr: "172.16.17.0/28"

# Internal (private) networking for cluster communication
yc_internal_network_name: "internal-ajasta-network"
yc_internal_subnet_name: "ajasta-internal-segment"
yc_internal_subnet_cidr: "10.10.0.0/24"

# Service account
yc_sa_name: "otus"

# Static IPs (one for master, one for worker)
master_address_name: "ajasta-k8s-master-ip"
worker1_address_name: "ajasta-k8s-worker1-ip"  # Kept for backward compatibility

# VM names
master_vm_name: "k8s-master"
worker1_vm_name: "k8s-worker-1"  # Kept for backward compatibility

# Worker nodes configuration (list-based for scalability)
# Each worker needs a unique vm_name and address_name
workers:
  - vm_name: "k8s-worker-1"
    address_name: "ajasta-k8s-worker1-ip"
  - vm_name: "k8s-worker-2"
    address_name: "ajasta-k8s-worker2-ip"
  - vm_name: "k8s-worker-3"
    address_name: "ajasta-k8s-worker3-ip"

# VM Resources (memory in GB, cores as integer)
# IMPORTANT: 6GB RAM is recommended for production workloads with storage backends
# - Master: control plane + etcd + API server + monitoring
# - Workers: application pods + storage backend (Longhorn/Rook-Ceph OSDs)
# Storage backend memory requirements:
#   - Longhorn: ~1GB per node
#   - Rook-Ceph OSDs: 2-4GB per OSD + mon/mgr overhead
# Cost consideration: 6GB provides stable operation for distributed storage systems
master_vm_memory: 6              # GB of RAM for master node (control plane + etcd + API server)
master_vm_cores: 2               # CPU cores for master node
master_vm_disk_size: 30          # GB of disk space for master node boot disk
worker_vm_memory: 6              # GB of RAM for worker nodes (sufficient for storage backends)
worker_vm_cores: 2               # CPU cores for worker nodes
worker_vm_disk_size: 30          # GB of disk space for worker node boot disk

# SSH
ssh_username: "ajasta"
ssh_pubkey_file: "~/.ssh/id_rsa.pub"

# Metadata (cloud-init)
# Use the standard metadata from scripts by default
metadata_yaml: "./scripts/metadata.yaml"

# Paths
scripts_dir: "./scripts"

# Rancher Dashboard
install_rancher: true
rancher_hostname: "rancher.local"

# Kubernetes bootstrap toggles and SSH retry tuning
# If false, playbook will provision VMs and networks only (no kubeadm, no Rancher)
bootstrap_k8s: true
# Use role-based bootstrap from ./other-k8s (recommended). Set to false to use legacy script-based bootstrap.
use_other_k8s: true
# Tune SSH retry logic used by setup-k8s-cluster.zsh
ssh_retry_attempts: 20         # Number of retry attempts for SSH connections
ssh_retry_delay: 6             # Delay in seconds between retry attempts
ssh_cmd_timeout: 300           # Maximum execution time in seconds for each SSH command (default: 300 = 5 minutes)

# Timeouts
rancher_install_timeout: 900
