---
- name: Deploy Ajasta Application to Kubernetes
  hosts: k8s_master
  gather_facts: false
  become: true
  
  vars:
    manifests_dir: "manifests"
    app_namespace: "ajasta"
    
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    
  tasks:
    - name: Check if kubectl is available
      command: kubectl version --client
      register: kubectl_check
      changed_when: false
      failed_when: false
      
    - name: Fail if kubectl is not available
      fail:
        msg: "kubectl is not available on the master node"
      when: kubectl_check.rc != 0
      
    - name: Check worker nodes disk space
      shell: |
        kubectl get nodes -o json | \
        jq -r '.items[] | select(.spec.taints == null or (.spec.taints | map(select(.key == "node-role.kubernetes.io/control-plane")) | length == 0)) | 
        .metadata.name + " " + (.status.conditions[] | select(.type == "DiskPressure") | .status)'
      register: disk_check
      changed_when: false
      failed_when: false
      
    - name: Get detailed node disk usage
      shell: |
        kubectl get nodes -o json | \
        jq -r '.items[] | select(.spec.taints == null or (.spec.taints | map(select(.key == "node-role.kubernetes.io/control-plane")) | length == 0)) |
        .metadata.name + " - Available: " + .status.allocatable."ephemeral-storage" + " - Capacity: " + .status.capacity."ephemeral-storage"'
      register: disk_usage
      changed_when: false
      failed_when: false
      
    - name: Display worker node disk status
      debug:
        msg: 
          - "=== Worker Node Disk Status ==="
          - "{{ disk_check.stdout_lines | default(['Unable to check disk pressure']) }}"
          - "=== Worker Node Disk Usage ==="
          - "{{ disk_usage.stdout_lines | default(['Unable to check disk usage']) }}"
          - ""
          - "⚠️  WARNING: If DiskPressure=True, you MUST clean up disk space before deployment!"
          - "   Run on worker nodes: sudo docker system prune -af && sudo rm -rf /var/lib/containerd/io.containerd.grpc.v1.cri/containers/*"
      
    - name: Fail if any worker node has disk pressure
      fail:
        msg: |
          CRITICAL: One or more worker nodes have disk pressure (DiskPressure=True)!
          
          You must clean up disk space on worker nodes before deployment:
          1. SSH to each affected worker node
          2. Run: sudo docker system prune -af
          3. Run: sudo crictl rmi --prune
          4. Check: df -h
          
          Disk pressure will cause pod evictions and deployment failures.
      when: disk_check.stdout is defined and "True" in disk_check.stdout
      
    - name: Check if nginx ingress controller is installed
      shell: kubectl get ingressclass nginx -o name 2>/dev/null || echo "not-found"
      register: nginx_ingress_check
      changed_when: false
      
    - name: Install nginx ingress controller
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
      when: nginx_ingress_check.stdout == "not-found"
      register: nginx_install
      
    - name: Wait for nginx ingress controller to be ready
      shell: |
        kubectl wait --namespace ingress-nginx \
          --for=condition=ready pod \
          --selector=app.kubernetes.io/component=controller \
          --timeout=300s
      when: nginx_ingress_check.stdout == "not-found"
      retries: 3
      delay: 10

    - name: Patch ingress-nginx controller Service with externalIPs (master public IP)
      shell: |
        MASTER_IP="{{ hostvars[groups['k8s_master'][0]].ansible_host | default('') }}"
        if [ -n "$MASTER_IP" ]; then
          kubectl -n ingress-nginx patch svc ingress-nginx-controller \
            --type merge \
            -p "{\"spec\":{\"externalIPs\":[\"$MASTER_IP\"]}}"
          echo "Added externalIPs [$MASTER_IP] to ingress-nginx-controller Service"
        else
          echo "Master public IP not found in inventory; skipping externalIPs patch"
        fi
      register: ingress_svc_patch
      changed_when: "Added externalIPs" in ingress_svc_patch.stdout
      failed_when: false

    - name: Show ingress-nginx controller Service summary
      shell: kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide
      register: ingress_svc_summary
      changed_when: false
      failed_when: false
      
    - name: Check if Longhorn is already installed
      shell: kubectl get namespace longhorn-system -o name 2>/dev/null || echo "not-found"
      register: longhorn_ns_check
      changed_when: false
      
    - name: Check if Longhorn StorageClass exists
      shell: kubectl get storageclass longhorn -o name 2>/dev/null || echo "not-found"
      register: longhorn_sc_check
      changed_when: false
      
    - name: Display Longhorn installation status
      debug:
        msg:
          - "Longhorn namespace: {{ longhorn_ns_check.stdout }}"
          - "Longhorn StorageClass: {{ longhorn_sc_check.stdout }}"
      
    - name: Determine if Longhorn needs setup or recovery
      set_fact:
        longhorn_needs_setup: "{{ longhorn_ns_check.stdout == 'not-found' or longhorn_sc_check.stdout == 'not-found' }}"
      
    - name: Display Longhorn setup requirement
      debug:
        msg: "Longhorn needs setup/recovery: {{ longhorn_needs_setup }}"
      
    - name: Delete failed Longhorn installation if namespace exists but StorageClass missing
      shell: |
        echo "Longhorn namespace exists but StorageClass is missing - cleaning up failed installation..."
        kubectl delete namespace longhorn-system --timeout=300s || true
        echo "Waiting for namespace deletion to complete..."
        sleep 10
      when: longhorn_ns_check.stdout != "not-found" and longhorn_sc_check.stdout == "not-found"
      register: longhorn_cleanup
      
    - name: Display Longhorn cleanup result
      debug:
        msg: "{{ longhorn_cleanup.stdout_lines }}"
      when: longhorn_cleanup is defined and longhorn_cleanup.stdout_lines is defined
      
    - name: Update namespace check after cleanup
      shell: kubectl get namespace longhorn-system -o name 2>/dev/null || echo "not-found"
      register: longhorn_ns_check
      changed_when: false
      when: longhorn_cleanup is defined and longhorn_cleanup.changed
      
    - name: Get worker nodes for Longhorn
      shell: |
        kubectl get nodes -o json | \
        jq -r '.items[] | select(.spec.taints == null or (.spec.taints | map(select(.key == "node-role.kubernetes.io/control-plane")) | length == 0)) | .metadata.name'
      register: worker_nodes
      changed_when: false
      when: longhorn_needs_setup | bool
      
    - name: Display worker nodes for Longhorn
      debug:
        msg: "Worker nodes that will use Longhorn: {{ worker_nodes.stdout_lines }}"
      when: longhorn_needs_setup | bool and worker_nodes.stdout_lines is defined
      
    - name: Check and install open-iscsi on worker nodes
      shell: |
        # Detect OS type
        if [ -f /etc/os-release ]; then
          . /etc/os-release
          OS_TYPE="$ID"
        else
          echo "ERROR: Cannot detect OS type"
          exit 1
        fi
        
        echo "=== Installing iSCSI initiator on $OS_TYPE ==="
        
        # Check if iscsiadm is already available
        if command -v iscsiadm >/dev/null 2>&1; then
          echo "✓ iscsiadm is already installed: $(iscsiadm --version 2>&1 | head -1)"
          # Ensure iscsid service is running
          systemctl start iscsid 2>/dev/null || true
          systemctl enable iscsid 2>/dev/null || true
          systemctl status iscsid --no-pager || echo "⚠ iscsid service status check failed"
          exit 0
        fi
        
        # Install based on OS type
        case "$OS_TYPE" in
          centos|rhel|rocky|almalinux|fedora)
            echo "Installing iscsi-initiator-utils for RHEL/CentOS-based system..."
            # Use dnf (preferred) or fall back to yum
            if command -v dnf >/dev/null 2>&1; then
              dnf install -y iscsi-initiator-utils
            else
              yum install -y iscsi-initiator-utils
            fi
            ;;
          ubuntu|debian)
            echo "Installing open-iscsi for Debian/Ubuntu-based system..."
            apt-get update && apt-get install -y open-iscsi
            ;;
          *)
            echo "ERROR: Unsupported OS type: $OS_TYPE"
            exit 1
            ;;
        esac
        
        # Verify installation
        if command -v iscsiadm >/dev/null 2>&1; then
          echo "✓ iscsiadm installed successfully: $(iscsiadm --version 2>&1 | head -1)"
        else
          echo "✗ ERROR: iscsiadm not found after installation!"
          exit 1
        fi
        
        # Enable and start iscsid service
        systemctl enable iscsid
        systemctl start iscsid
        
        # Verify service is running
        if systemctl is-active --quiet iscsid; then
          echo "✓ iscsid service is active and running"
        else
          echo "⚠ WARNING: iscsid service failed to start"
          systemctl status iscsid --no-pager
        fi
      delegate_to: "{{ item }}"
      loop: "{{ groups['k8s_workers'] }}"
      when: longhorn_needs_setup | bool
      register: iscsi_install
      become: true
      
    - name: Display open-iscsi installation results
      debug:
        msg: "{{ item.stdout_lines }}"
      loop: "{{ iscsi_install.results }}"
      when: longhorn_needs_setup | bool and iscsi_install is defined and iscsi_install.results is defined
      loop_control:
        label: "{{ item.item }}"
      
    - name: Check Longhorn prerequisites on worker nodes
      shell: |
        echo "=== Checking Longhorn Prerequisites ==="
        
        # Check iscsi_tcp kernel module
        if lsmod | grep -q iscsi_tcp; then
          echo "✓ iscsi_tcp module is loaded"
        else
          echo "✗ iscsi_tcp module is NOT loaded"
          echo "  Attempting to load iscsi_tcp module..."
          modprobe iscsi_tcp
          if lsmod | grep -q iscsi_tcp; then
            echo "✓ iscsi_tcp module loaded successfully"
          else
            echo "✗ FAILED to load iscsi_tcp module"
          fi
        fi
        
        # Check and create Longhorn directory
        if [ -d /var/lib/longhorn ]; then
          echo "✓ /var/lib/longhorn directory exists"
        else
          echo "  Creating /var/lib/longhorn directory..."
          mkdir -p /var/lib/longhorn
          echo "✓ /var/lib/longhorn directory created"
        fi
        
        # Set proper permissions
        chmod 755 /var/lib/longhorn
        echo "✓ /var/lib/longhorn permissions set to 755"
        
        # Check disk space
        df -h /var/lib/longhorn | tail -1
        
        # Check if multipathd is running (can cause conflicts)
        if systemctl is-active --quiet multipathd; then
          echo "⚠ multipathd is running (may cause conflicts with Longhorn)"
        else
          echo "✓ multipathd is not running"
        fi
        
        # Verify iscsid is running
        if systemctl is-active --quiet iscsid; then
          echo "✓ iscsid service is active"
        else
          echo "✗ iscsid service is NOT active"
        fi
        
        echo "=== Prerequisites check complete ==="
      delegate_to: "{{ item }}"
      loop: "{{ groups['k8s_workers'] }}"
      when: longhorn_needs_setup | bool
      register: longhorn_prereq_check
      become: true
      
    - name: Display Longhorn prerequisites check results
      debug:
        msg: "{{ item.stdout_lines }}"
      loop: "{{ longhorn_prereq_check.results }}"
      when: longhorn_needs_setup | bool and longhorn_prereq_check is defined and longhorn_prereq_check.results is defined
      loop_control:
        label: "{{ item.item }}"
      
    - name: Install Longhorn
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml
      when: longhorn_needs_setup | bool
      register: longhorn_install
      
    - name: Wait for Longhorn namespace to be created
      shell: kubectl get namespace longhorn-system -o name
      register: ns_created
      until: ns_created.rc == 0
      retries: 10
      delay: 5
      when: longhorn_needs_setup | bool
      
    - name: Wait for Longhorn DaemonSet to be created
      shell: kubectl get daemonset -n longhorn-system longhorn-manager -o name 2>/dev/null || echo "not-found"
      register: longhorn_ds_check
      until: longhorn_ds_check.stdout != "not-found"
      retries: 12
      delay: 5
      when: longhorn_needs_setup | bool
      changed_when: false
      
    - name: Get initial Longhorn manager pod status
      shell: kubectl get pods -n longhorn-system -l app=longhorn-manager -o wide
      register: initial_longhorn_status
      when: longhorn_needs_setup | bool
      changed_when: false
      failed_when: false
      
    - name: Display initial Longhorn manager pod status
      debug:
        msg: "{{ initial_longhorn_status.stdout_lines | default(['No pods found yet']) }}"
      when: longhorn_needs_setup | bool
      
    - name: Wait for Longhorn components to be ready
      block:
        - name: Check for early CrashLoopBackOff detection
          shell: |
            # Check if any pods are in CrashLoopBackOff state
            kubectl get pods -n longhorn-system -l app=longhorn-manager -o jsonpath='{.items[*].status.containerStatuses[*].state.waiting.reason}' | grep -q "CrashLoopBackOff"
          register: crashloop_check
          retries: 12
          delay: 10
          until: crashloop_check.rc != 0
          changed_when: false
          failed_when: false
          
        - name: Fail fast if CrashLoopBackOff detected
          fail:
            msg: "Longhorn manager pods are in CrashLoopBackOff state. Triggering diagnostics..."
          when: crashloop_check.rc == 0
          
        - name: Wait for Longhorn manager pods to be ready
          shell: |
            kubectl wait --namespace longhorn-system \
              --for=condition=ready pod \
              --selector=app=longhorn-manager \
              --timeout=600s
          retries: 2
          delay: 10
          
        - name: Wait for Longhorn driver deployer to be ready
          shell: |
            kubectl wait --namespace longhorn-system \
              --for=condition=ready pod \
              --selector=app=longhorn-driver-deployer \
              --timeout=300s
          retries: 2
          delay: 10
          
        - name: Wait for Longhorn StorageClass to be created
          shell: kubectl get storageclass longhorn -o name 2>/dev/null || echo "not-found"
          register: longhorn_sc_wait
          until: longhorn_sc_wait.stdout != "not-found"
          retries: 30
          delay: 10
          changed_when: false
          
        - name: Verify Longhorn StorageClass was created
          shell: kubectl get storageclass longhorn -o yaml
          register: longhorn_sc_details
          changed_when: false
          
        - name: Display Longhorn StorageClass details
          debug:
            msg: "{{ longhorn_sc_details.stdout_lines }}"
      when: longhorn_needs_setup | bool
      rescue:
        - name: Get Longhorn pods status on failure
          shell: kubectl get pods -n longhorn-system -o wide
          register: longhorn_pods_status
          changed_when: false
          failed_when: false
          
        - name: Describe Longhorn manager pods on failure
          shell: kubectl describe pod -n longhorn-system -l app=longhorn-manager
          register: longhorn_manager_describe
          changed_when: false
          failed_when: false
          
        - name: Get Longhorn manager logs on failure
          shell: |
            for pod in $(kubectl get pods -n longhorn-system -l app=longhorn-manager -o name); do
              echo "=== Current logs from $pod ==="
              kubectl logs -n longhorn-system $pod --tail=100 2>&1 || echo "Failed to get current logs"
              echo ""
              echo "=== Previous logs from $pod (if crashed) ==="
              kubectl logs -n longhorn-system $pod --previous --tail=100 2>&1 || echo "No previous logs (pod may not have crashed yet)"
              echo ""
            done
          register: longhorn_manager_logs
          changed_when: false
          failed_when: false
          
        - name: Check kernel modules on worker nodes
          shell: |
            echo "=== Kernel Modules Check ==="
            lsmod | grep -E "iscsi|nbd|dm_" || echo "No relevant modules found"
          delegate_to: "{{ item }}"
          loop: "{{ groups['k8s_workers'] }}"
          register: kernel_modules_check
          changed_when: false
          failed_when: false
          become: true
          
        - name: Get Longhorn events on failure
          shell: kubectl get events -n longhorn-system --sort-by='.lastTimestamp'
          register: longhorn_events
          changed_when: false
          failed_when: false
          
        - name: Check worker nodes status on failure
          shell: kubectl get nodes -o wide
          register: nodes_status
          changed_when: false
          failed_when: false
          
        - name: Check if open-iscsi is running on worker nodes
          shell: systemctl status iscsid --no-pager || echo "iscsid not running"
          delegate_to: "{{ item }}"
          loop: "{{ groups['k8s_workers'] }}"
          register: iscsi_status_check
          changed_when: false
          failed_when: false
          become: true
          
        - name: Check StorageClass status on failure
          shell: kubectl get storageclass -o wide
          register: storageclass_status_failure
          changed_when: false
          failed_when: false
          
        - name: Display Longhorn installation diagnostics
          debug:
            msg:
              - "=== Longhorn Pods Status ==="
              - "{{ longhorn_pods_status.stdout_lines | default(['No output']) }}"
              - ""
              - "=== Longhorn Manager Pod Description ==="
              - "{{ longhorn_manager_describe.stdout_lines | default(['No output']) }}"
              - ""
              - "=== Longhorn Manager Logs (Current + Previous) ==="
              - "{{ longhorn_manager_logs.stdout_lines | default(['No logs available']) }}"
              - ""
              - "=== Longhorn Events ==="
              - "{{ longhorn_events.stdout_lines | default(['No events']) }}"
              - ""
              - "=== Worker Nodes Status ==="
              - "{{ nodes_status.stdout_lines | default(['No output']) }}"
              - ""
              - "=== Kernel Modules on Workers ==="
              - "{% for result in kernel_modules_check.results %}{{ result.item }}: {{ result.stdout_lines | default(['Failed to check']) | join(' ') }}{% if not loop.last %}\n{% endif %}{% endfor %}"
              - ""
              - "=== open-iscsi Status on Workers ==="
              - "{% for result in iscsi_status_check.results %}{{ result.item }}: {{ result.stdout_lines | default(['Failed to check']) | join(' ') }}{% if not loop.last %}\n{% endif %}{% endfor %}"
              - ""
              - "=== StorageClass Status ==="
              - "{{ storageclass_status_failure.stdout_lines | default(['No StorageClass found']) }}"
              
        - name: Fail after Longhorn diagnostics
          fail:
            msg: |
              Longhorn manager pods failed to become ready. See diagnostics above.
              
              Common causes:
              1. open-iscsi not installed/running: Ensure iscsid service is active on all worker nodes
              2. Insufficient resources: Worker nodes may not have enough CPU/memory
              3. Disk issues: Check if worker nodes have available disk space
              4. Network issues: Longhorn requires network connectivity between nodes
              5. Missing kernel modules: iscsi_tcp module may not be loaded
              
              To fix:
              - Verify open-iscsi on workers: ssh to each worker and run 'systemctl status iscsid'
              - Check worker resources: kubectl describe nodes
              - Check Longhorn manager logs above for specific errors
              - Ensure iscsi_tcp kernel module is loaded: lsmod | grep iscsi_tcp
              - Check Longhorn system requirements: https://longhorn.io/docs/1.5.3/deploy/install/#installation-requirements
      
    - name: Check if Longhorn StorageClass exists (final verification)
      shell: kubectl get storageclass longhorn -o name 2>/dev/null || echo "not-found"
      register: longhorn_sc_final_check
      changed_when: false
      
    - name: Set Longhorn as default StorageClass
      shell: |
        kubectl patch storageclass longhorn -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
      when: longhorn_sc_final_check.stdout != "not-found"
      register: longhorn_default_set
      changed_when: longhorn_default_set.rc == 0
      
    - name: Display StorageClass default setting result
      debug:
        msg: "Longhorn StorageClass set as default"
      when: longhorn_sc_final_check.stdout != "not-found"
      
    - name: Warn if StorageClass was not found
      debug:
        msg: "WARNING: Longhorn StorageClass not found! PostgreSQL deployment may fail."
      when: longhorn_sc_final_check.stdout == "not-found"
      
    - name: Verify Longhorn installation
      shell: |
        kubectl get pods -n longhorn-system
      register: longhorn_status
      changed_when: false
      failed_when: false
      
    - name: Display Longhorn pod status
      debug:
        msg: "{{ longhorn_status.stdout_lines }}"
      
    - name: Verify StorageClass availability before PostgreSQL deployment
      shell: kubectl get storageclass longhorn -o jsonpath='{.provisioner}'
      register: storageclass_provisioner
      changed_when: false
      failed_when: false
      
    - name: Fail if StorageClass is not available
      fail:
        msg: |
          CRITICAL: Longhorn StorageClass does not exist!
          PostgreSQL requires a StorageClass to provision persistent volumes.
          
          The StorageClass should have been created by Longhorn manager pods.
          Check Longhorn installation logs above for errors.
          
          Common causes:
          1. Longhorn manager pods are not running or in CrashLoopBackOff state
          2. open-iscsi not properly installed on worker nodes
          3. Insufficient resources on worker nodes
          
          To fix:
          - Check Longhorn pods: kubectl get pods -n longhorn-system
          - Check Longhorn manager logs: kubectl logs -n longhorn-system -l app=longhorn-manager
          - Verify open-iscsi on all workers: systemctl status iscsid
      when: storageclass_provisioner.rc != 0 or storageclass_provisioner.stdout == ""
      
    - name: Display StorageClass verification result
      debug:
        msg: "✓ Longhorn StorageClass is available (provisioner: {{ storageclass_provisioner.stdout }})"
      when: storageclass_provisioner.rc == 0 and storageclass_provisioner.stdout != ""
      
    - name: Copy manifests directory to master node
      copy:
        src: "{{ manifests_dir }}/"
        dest: "/tmp/ajasta-manifests/"
        mode: '0644'
        
    - name: Apply namespace
      command: kubectl apply -f /tmp/ajasta-manifests/01-namespace.yml
      register: namespace_result
      changed_when: "'created' in namespace_result.stdout or 'configured' in namespace_result.stdout"
      
    - name: Apply PostgreSQL secret
      command: kubectl apply -f /tmp/ajasta-manifests/02-postgres-secret.yml
      register: postgres_secret_result
      changed_when: "'created' in postgres_secret_result.stdout or 'configured' in postgres_secret_result.stdout"
      
    - name: Check if PostgreSQL StatefulSet exists
      shell: kubectl get statefulset ajasta-postgres -n {{ app_namespace }} -o name 2>/dev/null || echo "not-found"
      register: postgres_statefulset_check
      changed_when: false
      
    - name: Delete existing PostgreSQL StatefulSet (if exists) to handle immutable field updates
      shell: |
        kubectl delete statefulset ajasta-postgres -n {{ app_namespace }} --cascade=orphan
      when: postgres_statefulset_check.stdout != "not-found"
      register: postgres_statefulset_delete
      changed_when: postgres_statefulset_delete.rc == 0
      
    - name: Apply PostgreSQL StatefulSet
      command: kubectl apply -f /tmp/ajasta-manifests/03-postgres-statefulset.yml
      register: postgres_statefulset_result
      changed_when: "'created' in postgres_statefulset_result.stdout or 'configured' in postgres_statefulset_result.stdout"
      
    - name: Apply PostgreSQL Service
      command: kubectl apply -f /tmp/ajasta-manifests/04-postgres-service.yml
      register: postgres_service_result
      changed_when: "'created' in postgres_service_result.stdout or 'configured' in postgres_service_result.stdout"
      
    - name: Check PostgreSQL PVC status
      shell: kubectl get pvc -n {{ app_namespace }}
      register: pvc_status
      changed_when: false
      failed_when: false
      
    - name: Display PVC status
      debug:
        msg: "{{ pvc_status.stdout_lines }}"
      when: pvc_status.stdout is defined
      
    - name: Wait for PostgreSQL to be ready
      block:
        - name: Wait for PostgreSQL pod
          shell: |
            kubectl wait --namespace {{ app_namespace }} \
              --for=condition=ready pod \
              --selector=app=ajasta,component=database \
              --timeout=600s
          retries: 2
          delay: 15
      rescue:
        - name: Get PostgreSQL pod status on failure
          shell: kubectl get pods -n {{ app_namespace }} -l component=database
          register: postgres_pod_status
          changed_when: false
          failed_when: false
          
        - name: Describe PostgreSQL pod on failure
          shell: kubectl describe pod -n {{ app_namespace }} -l component=database
          register: postgres_pod_describe
          changed_when: false
          failed_when: false
          
        - name: Get PostgreSQL pod logs on failure
          shell: kubectl logs -n {{ app_namespace }} -l component=database --tail=100
          register: postgres_pod_logs
          changed_when: false
          failed_when: false
          
        - name: Get PVC status on failure
          shell: kubectl get pvc -n {{ app_namespace }}
          register: postgres_pvc_status
          changed_when: false
          failed_when: false
          
        - name: Describe PVC on failure
          shell: kubectl describe pvc -n {{ app_namespace }}
          register: postgres_pvc_describe
          changed_when: false
          failed_when: false
          
        - name: Get PV list on failure
          shell: kubectl get pv
          register: postgres_pv_list
          changed_when: false
          failed_when: false
          
        - name: Get StorageClass details on failure
          shell: kubectl get storageclass -o wide
          register: postgres_sc_status
          changed_when: false
          failed_when: false
          
        - name: Get Longhorn manager status on failure
          shell: kubectl get pods -n longhorn-system -l app=longhorn-manager
          register: postgres_provisioner_status
          changed_when: false
          failed_when: false
          
        - name: Describe Longhorn manager on failure
          shell: kubectl describe pod -n longhorn-system -l app=longhorn-manager
          register: postgres_provisioner_describe
          changed_when: false
          failed_when: false
          
        - name: Display PostgreSQL diagnostics
          debug:
            msg:
              - "=== PostgreSQL Pod Status ==="
              - "{{ postgres_pod_status.stdout_lines | default(['No output']) }}"
              - ""
              - "=== PVC Status ==="
              - "{{ postgres_pvc_status.stdout_lines | default(['No PVCs found']) }}"
              - ""
              - "=== PVC Description ==="
              - "{{ postgres_pvc_describe.stdout_lines | default(['No PVC details']) }}"
              - ""
              - "=== PersistentVolumes ==="
              - "{{ postgres_pv_list.stdout_lines | default(['No PVs found']) }}"
              - ""
              - "=== StorageClasses ==="
              - "{{ postgres_sc_status.stdout_lines | default(['No StorageClasses found']) }}"
              - ""
              - "=== Longhorn Manager Status ==="
              - "{{ postgres_provisioner_status.stdout_lines | default(['Longhorn manager not found']) }}"
              - ""
              - "=== Longhorn Manager Details ==="
              - "{{ postgres_provisioner_describe.stdout_lines | default(['No Longhorn manager details']) }}"
              - ""
              - "=== PostgreSQL Pod Description ==="
              - "{{ postgres_pod_describe.stdout_lines | default(['No output']) }}"
              - ""
              - "=== PostgreSQL Pod Logs ==="
              - "{{ postgres_pod_logs.stdout_lines | default(['No logs available']) }}"
              
        - name: Fail after diagnostics
          fail:
            msg: |
              PostgreSQL pod failed to become ready. See diagnostics above.
              
              Common causes:
              1. PVC not bound: Check if Longhorn manager pods are running
              2. No PersistentVolume: Longhorn may not be creating PVs
              3. StorageClass missing: Ensure 'longhorn' StorageClass exists
              4. Insufficient disk space: Check worker node disk usage
              5. Missing dependencies: Ensure open-iscsi is installed on worker nodes
              
              To fix:
              - Ensure Longhorn is running: kubectl get pods -n longhorn-system
              - Check Longhorn manager logs: kubectl logs -n longhorn-system -l app=longhorn-manager
              - Verify StorageClass: kubectl get storageclass longhorn -o yaml
              - Check node disk space: kubectl describe nodes
              - Verify open-iscsi: SSH to worker nodes and check 'systemctl status iscsid'
      
    - name: Apply Backend secret
      command: kubectl apply -f /tmp/ajasta-manifests/05-backend-secret.yml
      register: backend_secret_result
      changed_when: "'created' in backend_secret_result.stdout or 'configured' in backend_secret_result.stdout"
      
    - name: Apply Backend ConfigMap
      command: kubectl apply -f /tmp/ajasta-manifests/06-backend-configmap.yml
      register: backend_config_result
      changed_when: "'created' in backend_config_result.stdout or 'configured' in backend_config_result.stdout"
      
    - name: Apply Backend Deployment
      command: kubectl apply -f /tmp/ajasta-manifests/07-backend-deployment.yml
      register: backend_deployment_result
      changed_when: "'created' in backend_deployment_result.stdout or 'configured' in backend_deployment_result.stdout"
      
    - name: Apply Backend Service
      command: kubectl apply -f /tmp/ajasta-manifests/08-backend-service.yml
      register: backend_service_result
      changed_when: "'created' in backend_service_result.stdout or 'configured' in backend_service_result.stdout"
      
    - name: Wait for Backend to be ready
      block:
        - name: Wait for Backend pod
          shell: |
            kubectl wait --namespace {{ app_namespace }} \
              --for=condition=ready pod \
              --selector=app=ajasta,component=backend \
              --timeout=600s
          retries: 2
          delay: 15
      rescue:
        - name: Get Backend pod status on failure
          shell: kubectl get pods -n {{ app_namespace }} -l component=backend
          register: backend_pod_status
          changed_when: false
          failed_when: false
          
        - name: Describe Backend pod on failure
          shell: kubectl describe pod -n {{ app_namespace }} -l component=backend
          register: backend_pod_describe
          changed_when: false
          failed_when: false
          
        - name: Get Backend pod logs on failure
          shell: kubectl logs -n {{ app_namespace }} -l component=backend --tail=100
          register: backend_pod_logs
          changed_when: false
          failed_when: false
          
        - name: Display Backend diagnostics
          debug:
            msg:
              - "=== Backend Pod Status ==="
              - "{{ backend_pod_status.stdout_lines | default(['No output']) }}"
              - "=== Backend Pod Description ==="
              - "{{ backend_pod_describe.stdout_lines | default(['No output']) }}"
              - "=== Backend Pod Logs ==="
              - "{{ backend_pod_logs.stdout_lines | default(['No logs available']) }}"
              
        - name: Fail after diagnostics
          fail:
            msg: "Backend pod failed to become ready. See diagnostics above."
      
    - name: Apply Frontend Deployment
      command: kubectl apply -f /tmp/ajasta-manifests/09-frontend-deployment.yml
      register: frontend_deployment_result
      changed_when: "'created' in frontend_deployment_result.stdout or 'configured' in frontend_deployment_result.stdout"
      
    - name: Apply Frontend Service
      command: kubectl apply -f /tmp/ajasta-manifests/10-frontend-service.yml
      register: frontend_service_result
      changed_when: "'created' in frontend_service_result.stdout or 'configured' in frontend_service_result.stdout"
      
    - name: Wait for Frontend to be ready
      shell: |
        kubectl wait --namespace {{ app_namespace }} \
          --for=condition=ready pod \
          --selector=app=ajasta,component=frontend \
          --timeout=300s
      retries: 3
      delay: 10
      
    - name: Apply Ingress
      command: kubectl apply -f /tmp/ajasta-manifests/11-ingress.yml
      register: ingress_result
      changed_when: "'created' in ingress_result.stdout or 'configured' in ingress_result.stdout"
      
    - name: Wait for Ingress to get an external IP
      shell: |
        for i in {1..60}; do
          IP=$(kubectl get ingress ajasta-ingress -n {{ app_namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
          if [ -n "$IP" ]; then
            echo "Ingress IP: $IP"
            exit 0
          fi
          sleep 5
        done
        echo "Timeout waiting for Ingress IP"
        exit 1
      register: ingress_ip
      failed_when: false
      
    - name: Get deployment status
      shell: kubectl get all -n {{ app_namespace }}
      register: deployment_status
      changed_when: false
      
    - name: Display deployment status
      debug:
        msg: "{{ deployment_status.stdout_lines }}"
        
    - name: Display Ingress information
      shell: kubectl get ingress -n {{ app_namespace }}
      register: ingress_info
      changed_when: false
      
    - name: Show Ingress details
      debug:
        msg: "{{ ingress_info.stdout_lines }}"
        
    - name: Cleanup temporary manifests
      file:
        path: "/tmp/ajasta-manifests/"
        state: absent
