---
- name: Deploy Ajasta Application to Kubernetes
  hosts: k8s_master
  gather_facts: false
  become: true
  
  vars:
    manifests_dir: "manifests"
    app_namespace: "ajasta"
    # Storage backend to use: "longhorn" or "rook"
    storage_backend: "rook"
    
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
    
  tasks:
    - name: Check if kubectl is available
      command: kubectl version --client
      register: kubectl_check
      changed_when: false
      failed_when: false
      
    - name: Fail if kubectl is not available
      fail:
        msg: "kubectl is not available on the master node"
      when: kubectl_check.rc != 0
      
    - name: Check worker nodes disk space
      shell: |
        kubectl get nodes -o json | \
        jq -r '.items[] | select(.spec.taints == null or (.spec.taints | map(select(.key == "node-role.kubernetes.io/control-plane")) | length == 0)) | 
        .metadata.name + " " + (.status.conditions[] | select(.type == "DiskPressure") | .status)'
      register: disk_check
      changed_when: false
      failed_when: false
      
    - name: Get detailed node disk usage
      shell: |
        kubectl get nodes -o json | \
        jq -r '.items[] | select(.spec.taints == null or (.spec.taints | map(select(.key == "node-role.kubernetes.io/control-plane")) | length == 0)) |
        .metadata.name + " - Available: " + .status.allocatable."ephemeral-storage" + " - Capacity: " + .status.capacity."ephemeral-storage"'
      register: disk_usage
      changed_when: false
      failed_when: false
      
    - name: Display worker node disk status
      debug:
        msg: 
          - "=== Worker Node Disk Status ==="
          - "{{ disk_check.stdout_lines | default(['Unable to check disk pressure']) }}"
          - "=== Worker Node Disk Usage ==="
          - "{{ disk_usage.stdout_lines | default(['Unable to check disk usage']) }}"
          - ""
          - "⚠️  WARNING: If DiskPressure=True, you MUST clean up disk space before deployment!"
          - "   Run on worker nodes: sudo docker system prune -af && sudo rm -rf /var/lib/containerd/io.containerd.grpc.v1.cri/containers/*"
      
    - name: Fail if any worker node has disk pressure
      fail:
        msg: |
          CRITICAL: One or more worker nodes have disk pressure (DiskPressure=True)!
          
          You must clean up disk space on worker nodes before deployment:
          1. SSH to each affected worker node
          2. Run: sudo docker system prune -af
          3. Run: sudo crictl rmi --prune
          4. Check: df -h
          
          Disk pressure will cause pod evictions and deployment failures.
      when: disk_check.stdout is defined and "True" in disk_check.stdout
      
    - name: Check if nginx ingress controller is installed
      shell: kubectl get ingressclass nginx -o name 2>/dev/null || echo "not-found"
      register: nginx_ingress_check
      changed_when: false
      
    - name: Install nginx ingress controller
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
      when: nginx_ingress_check.stdout == "not-found"
      register: nginx_install
      
    - name: Wait for nginx ingress controller to be ready
      shell: |
        kubectl wait --namespace ingress-nginx \
          --for=condition=ready pod \
          --selector=app.kubernetes.io/component=controller \
          --timeout=300s
      when: nginx_ingress_check.stdout == "not-found"
      retries: 3
      delay: 10

    - name: Patch ingress-nginx controller Service with externalIPs (master public IP)
      shell: |
        MASTER_IP="{{ hostvars[groups['k8s_master'][0]].ansible_host | default('') }}"
        if [ -n "$MASTER_IP" ]; then
          kubectl -n ingress-nginx patch svc ingress-nginx-controller \
            --type merge \
            -p "{\"spec\":{\"externalIPs\":[\"$MASTER_IP\"]}}"
          echo "Added externalIPs [$MASTER_IP] to ingress-nginx-controller Service"
        else
          echo "Master public IP not found in inventory; skipping externalIPs patch"
        fi
      register: ingress_svc_patch
      changed_when: '"Added externalIPs" in ingress_svc_patch.stdout'
      failed_when: false

    - name: Show ingress-nginx controller Service summary
      shell: kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide
      register: ingress_svc_summary
      changed_when: false
      failed_when: false
      
    - name: Display selected storage backend
      debug:
        msg: 
          - "=== Storage Backend Selection ==="
          - "Selected storage backend: {{ storage_backend }}"
          - "{% if storage_backend == 'longhorn' %}Installing Longhorn distributed block storage{% elif storage_backend == 'rook' %}Installing Rook-Ceph distributed storage{% else %}Unknown storage backend!{% endif %}"
      
    - name: Validate storage backend parameter
      fail:
        msg: "Invalid storage_backend value: '{{ storage_backend }}'. Must be 'longhorn' or 'rook'."
      when: storage_backend not in ['longhorn', 'rook']
      
    - name: Install Longhorn storage backend
      include_role:
        name: storage_longhorn
      when: storage_backend == 'longhorn'
      
    - name: Ensure Longhorn is installed (required for Rook OSD PVCs)
      include_role:
        name: storage_longhorn
      when: storage_backend == 'rook'

    - name: Install Rook storage backend
      include_role:
        name: storage_rook
      when: storage_backend == 'rook'
      
    - name: Copy manifests directory to master node
      copy:
        src: "{{ manifests_dir }}/"
        dest: "/tmp/ajasta-manifests/"
        mode: '0644'
        
    - name: Apply namespace
      command: kubectl apply -f /tmp/ajasta-manifests/01-namespace.yml
      register: namespace_result
      changed_when: "'created' in namespace_result.stdout or 'configured' in namespace_result.stdout"
      
    - name: Apply PostgreSQL secret
      command: kubectl apply -f /tmp/ajasta-manifests/02-postgres-secret.yml
      register: postgres_secret_result
      changed_when: "'created' in postgres_secret_result.stdout or 'configured' in postgres_secret_result.stdout"
      
    - name: Check if PostgreSQL StatefulSet exists
      shell: kubectl get statefulset ajasta-postgres -n {{ app_namespace }} -o name 2>/dev/null || echo "not-found"
      register: postgres_statefulset_check
      changed_when: false
      
    - name: Delete existing PostgreSQL StatefulSet (if exists) to handle immutable field updates
      shell: |
        kubectl delete statefulset ajasta-postgres -n {{ app_namespace }} --cascade=orphan
      when: postgres_statefulset_check.stdout != "not-found"
      register: postgres_statefulset_delete
      changed_when: postgres_statefulset_delete.rc == 0
      
    - name: Apply PostgreSQL StatefulSet
      command: kubectl apply -f /tmp/ajasta-manifests/03-postgres-statefulset.yml
      register: postgres_statefulset_result
      changed_when: "'created' in postgres_statefulset_result.stdout or 'configured' in postgres_statefulset_result.stdout"
      
    - name: Apply PostgreSQL Service
      command: kubectl apply -f /tmp/ajasta-manifests/04-postgres-service.yml
      register: postgres_service_result
      changed_when: "'created' in postgres_service_result.stdout or 'configured' in postgres_service_result.stdout"
      
    - name: Check PostgreSQL PVC status
      shell: kubectl get pvc -n {{ app_namespace }}
      register: pvc_status
      changed_when: false
      failed_when: false
      
    - name: Display PVC status
      debug:
        msg: "{{ pvc_status.stdout_lines }}"
      when: pvc_status.stdout is defined
      
    - name: Wait for PostgreSQL to be ready
      block:
        - name: Wait for PostgreSQL pod
          shell: |
            kubectl wait --namespace {{ app_namespace }} \
              --for=condition=ready pod \
              --selector=app=ajasta,component=database \
              --timeout=600s
          retries: 2
          delay: 15
      rescue:
        - name: Get PostgreSQL pod status on failure
          shell: kubectl get pods -n {{ app_namespace }} -l component=database
          register: postgres_pod_status
          changed_when: false
          failed_when: false
          
        - name: Describe PostgreSQL pod on failure
          shell: kubectl describe pod -n {{ app_namespace }} -l component=database
          register: postgres_pod_describe
          changed_when: false
          failed_when: false
          
        - name: Get PostgreSQL pod logs on failure
          shell: kubectl logs -n {{ app_namespace }} -l component=database --tail=100
          register: postgres_pod_logs
          changed_when: false
          failed_when: false
          
        - name: Get PVC status on failure
          shell: kubectl get pvc -n {{ app_namespace }}
          register: postgres_pvc_status
          changed_when: false
          failed_when: false
          
        - name: Describe PVC on failure
          shell: kubectl describe pvc -n {{ app_namespace }}
          register: postgres_pvc_describe
          changed_when: false
          failed_when: false
          
        - name: Get PV list on failure
          shell: kubectl get pv
          register: postgres_pv_list
          changed_when: false
          failed_when: false
          
        - name: Get StorageClass details on failure
          shell: kubectl get storageclass -o wide
          register: postgres_sc_status
          changed_when: false
          failed_when: false
          
        - name: Get storage provisioner status on failure (Longhorn)
          shell: kubectl get pods -n longhorn-system -l app=longhorn-manager
          register: postgres_provisioner_status
          changed_when: false
          failed_when: false
          when: storage_backend == 'longhorn'
          
        - name: Describe storage provisioner on failure (Longhorn)
          shell: kubectl describe pod -n longhorn-system -l app=longhorn-manager
          register: postgres_provisioner_describe
          changed_when: false
          failed_when: false
          when: storage_backend == 'longhorn'
          
        - name: Get storage provisioner status on failure (Rook)
          shell: kubectl get pods -n rook-ceph -l app=rook-ceph-operator
          register: postgres_provisioner_status
          changed_when: false
          failed_when: false
          when: storage_backend == 'rook'
          
        - name: Get Rook CSI provisioner status on failure (Rook)
          shell: kubectl get pods -n rook-ceph -l app=csi-rbdplugin-provisioner
          register: postgres_csi_provisioner_status
          changed_when: false
          failed_when: false
          when: storage_backend == 'rook'
          
        - name: Describe storage provisioner on failure (Rook)
          shell: kubectl describe pod -n rook-ceph -l app=csi-rbdplugin-provisioner
          register: postgres_provisioner_describe
          changed_when: false
          failed_when: false
          when: storage_backend == 'rook'
          
        - name: Display PostgreSQL diagnostics
          debug:
            msg:
              - "=== PostgreSQL Pod Status ==="
              - "{{ postgres_pod_status.stdout_lines | default(['No output']) }}"
              - ""
              - "=== PVC Status ==="
              - "{{ postgres_pvc_status.stdout_lines | default(['No PVCs found']) }}"
              - ""
              - "=== PVC Description ==="
              - "{{ postgres_pvc_describe.stdout_lines | default(['No PVC details']) }}"
              - ""
              - "=== PersistentVolumes ==="
              - "{{ postgres_pv_list.stdout_lines | default(['No PVs found']) }}"
              - ""
              - "=== StorageClasses ==="
              - "{{ postgres_sc_status.stdout_lines | default(['No StorageClasses found']) }}"
              - ""
              - "=== Storage Backend: {{ storage_backend | upper }} ==="
              - "{% if storage_backend == 'longhorn' %}=== Longhorn Manager Status ==={{ '\n' }}{{ postgres_provisioner_status.stdout_lines | default(['Longhorn manager not found']) | join('\n') }}{{ '\n' }}{{ '\n' }}=== Longhorn Manager Details ==={{ '\n' }}{{ postgres_provisioner_describe.stdout_lines | default(['No Longhorn manager details']) | join('\n') }}{% elif storage_backend == 'rook' %}=== Rook Operator Status ==={{ '\n' }}{{ postgres_provisioner_status.stdout_lines | default(['Rook operator not found']) | join('\n') }}{{ '\n' }}{{ '\n' }}=== Rook CSI Provisioner Status ==={{ '\n' }}{{ postgres_csi_provisioner_status.stdout_lines | default(['Rook CSI provisioner not found']) | join('\n') }}{{ '\n' }}{{ '\n' }}=== Rook CSI Provisioner Details ==={{ '\n' }}{{ postgres_provisioner_describe.stdout_lines | default(['No Rook CSI provisioner details']) | join('\n') }}{% endif %}"
              - ""
              - "=== PostgreSQL Pod Description ==="
              - "{{ postgres_pod_describe.stdout_lines | default(['No output']) }}"
              - ""
              - "=== PostgreSQL Pod Logs ==="
              - "{{ postgres_pod_logs.stdout_lines | default(['No logs available']) }}"
              
        - name: Fail after diagnostics
          fail:
            msg: |
              PostgreSQL pod failed to become ready. See diagnostics above.
              
              Storage Backend: {{ storage_backend | upper }}
              
              Common causes:
              1. PVC not bound: Check if storage provisioner pods are running
              2. No PersistentVolume: Storage backend may not be creating PVs
              3. StorageClass missing: Ensure 'longhorn' StorageClass exists
              4. Insufficient disk space: Check worker node disk usage
              {% if storage_backend == 'longhorn' %}5. Missing dependencies: Ensure open-iscsi is installed on worker nodes{% elif storage_backend == 'rook' %}5. CSI driver not ready: Rook CSI provisioner may not be operational{% endif %}
              
              {% if storage_backend == 'longhorn' %}To fix (Longhorn):
              - Ensure Longhorn is running: kubectl get pods -n longhorn-system
              - Check Longhorn manager logs: kubectl logs -n longhorn-system -l app=longhorn-manager
              - Verify StorageClass: kubectl get storageclass longhorn -o yaml
              - Check node disk space: kubectl describe nodes
              - Verify open-iscsi: SSH to worker nodes and check 'systemctl status iscsid'
              {% elif storage_backend == 'rook' %}To fix (Rook):
              - Ensure Rook operator is running: kubectl get pods -n rook-ceph -l app=rook-ceph-operator
              - Check Rook CSI provisioner: kubectl get pods -n rook-ceph -l app=csi-rbdplugin-provisioner
              - Check Ceph cluster health: kubectl get cephcluster -n rook-ceph
              - Check CSI provisioner logs: kubectl logs -n rook-ceph -l app=csi-rbdplugin-provisioner
              - Verify StorageClass: kubectl get storageclass longhorn -o yaml
              - Check Ceph OSD pods: kubectl get pods -n rook-ceph -l app=rook-ceph-osd
              - Check node disk space: kubectl describe nodes
              {% endif %}
      
    - name: Apply Backend secret
      command: kubectl apply -f /tmp/ajasta-manifests/05-backend-secret.yml
      register: backend_secret_result
      changed_when: "'created' in backend_secret_result.stdout or 'configured' in backend_secret_result.stdout"
      
    - name: Apply Backend ConfigMap
      command: kubectl apply -f /tmp/ajasta-manifests/06-backend-configmap.yml
      register: backend_config_result
      changed_when: "'created' in backend_config_result.stdout or 'configured' in backend_config_result.stdout"
      
    - name: Apply Backend Deployment
      command: kubectl apply -f /tmp/ajasta-manifests/07-backend-deployment.yml
      register: backend_deployment_result
      changed_when: "'created' in backend_deployment_result.stdout or 'configured' in backend_deployment_result.stdout"
      
    - name: Apply Backend Service
      command: kubectl apply -f /tmp/ajasta-manifests/08-backend-service.yml
      register: backend_service_result
      changed_when: "'created' in backend_service_result.stdout or 'configured' in backend_service_result.stdout"
      
    - name: Wait for Backend to be ready
      block:
        - name: Wait for Backend pod
          shell: |
            kubectl wait --namespace {{ app_namespace }} \
              --for=condition=ready pod \
              --selector=app=ajasta,component=backend \
              --timeout=600s
          retries: 2
          delay: 15
      rescue:
        - name: Get Backend pod status on failure
          shell: kubectl get pods -n {{ app_namespace }} -l component=backend
          register: backend_pod_status
          changed_when: false
          failed_when: false
          
        - name: Describe Backend pod on failure
          shell: kubectl describe pod -n {{ app_namespace }} -l component=backend
          register: backend_pod_describe
          changed_when: false
          failed_when: false
          
        - name: Get Backend pod logs on failure
          shell: kubectl logs -n {{ app_namespace }} -l component=backend --tail=100
          register: backend_pod_logs
          changed_when: false
          failed_when: false
          
        - name: Display Backend diagnostics
          debug:
            msg:
              - "=== Backend Pod Status ==="
              - "{{ backend_pod_status.stdout_lines | default(['No output']) }}"
              - "=== Backend Pod Description ==="
              - "{{ backend_pod_describe.stdout_lines | default(['No output']) }}"
              - "=== Backend Pod Logs ==="
              - "{{ backend_pod_logs.stdout_lines | default(['No logs available']) }}"
              
        - name: Fail after diagnostics
          fail:
            msg: "Backend pod failed to become ready. See diagnostics above."
      
    - name: Apply Frontend Deployment
      command: kubectl apply -f /tmp/ajasta-manifests/09-frontend-deployment.yml
      register: frontend_deployment_result
      changed_when: "'created' in frontend_deployment_result.stdout or 'configured' in frontend_deployment_result.stdout"

    - name: Ensure frontend uses ClusterIP Service (or override)
      shell: |
        kubectl -n {{ app_namespace }} set env deploy/ajasta-frontend \
          FRONTEND_API_BASE="{{ frontend_api_base | default('http://ajasta-backend:8090/api') }}"
      register: frontend_env_result
      changed_when: false
      failed_when: false
      
    - name: Apply Frontend Service
      command: kubectl apply -f /tmp/ajasta-manifests/10-frontend-service.yml
      register: frontend_service_result
      changed_when: "'created' in frontend_service_result.stdout or 'configured' in frontend_service_result.stdout"
      
    - name: Force rollout restart of frontend (ensure initContainer rewrite runs)
      shell: kubectl -n {{ app_namespace }} rollout restart deploy/ajasta-frontend
      register: frontend_rollout_restart
      changed_when: false
      failed_when: false
      
    - name: Wait for Frontend to be ready
      shell: |
        kubectl wait --namespace {{ app_namespace }} \
          --for=condition=ready pod \
          --selector=app=ajasta,component=frontend \
          --timeout=300s
      retries: 3
      delay: 10

    - name: Guard check — ensure no leftover ${external_ip} in served assets
      shell: |
        set -e
        POD=$(kubectl -n {{ app_namespace }} get pods -l app=ajasta,component=frontend -o name | head -n1)
        if [ -z "$POD" ]; then echo "no_frontend_pod"; exit 1; fi
        kubectl -n {{ app_namespace }} exec "$POD" -- sh -lc 'grep -R "external_ip" -n /usr/share/nginx/html || true'
      register: frontend_external_ip_grep
      changed_when: false
      failed_when: false

    - name: Fail if ${external_ip} placeholder still present in frontend assets
      fail:
        msg: |
          Detected leftover ${external_ip} references in frontend assets after rollout. This breaks API calls (ERR_NAME_NOT_RESOLVED).
          Offending lines:\n{{ frontend_external_ip_grep.stdout }}
          Remediation:
          - Ensure FRONTEND_API_BASE is set to http://ajasta-backend:8090/api
          - Re-apply 09-frontend-deployment.yml and rerun this play
      when: frontend_external_ip_grep.stdout is defined and (frontend_external_ip_grep.stdout | trim) != ""
      
    - name: Apply Ingress
      command: kubectl apply -f /tmp/ajasta-manifests/11-ingress.yml
      register: ingress_result
      changed_when: "'created' in ingress_result.stdout or 'configured' in ingress_result.stdout"
      
    - name: Wait for Ingress to get an external IP
      shell: |
        for i in {1..60}; do
          IP=$(kubectl get ingress ajasta-ingress -n {{ app_namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
          if [ -n "$IP" ]; then
            echo "Ingress IP: $IP"
            exit 0
          fi
          sleep 5
        done
        echo "Timeout waiting for Ingress IP"
        exit 1
      register: ingress_ip
      failed_when: false
      
    - name: Get deployment status
      shell: kubectl get all -n {{ app_namespace }}
      register: deployment_status
      changed_when: false
      
    - name: Display deployment status
      debug:
        msg: "{{ deployment_status.stdout_lines }}"
        
    - name: Display Ingress information
      shell: kubectl get ingress -n {{ app_namespace }}
      register: ingress_info
      changed_when: false
      
    - name: Show Ingress details
      debug:
        msg: "{{ ingress_info.stdout_lines }}"
        
    - name: Cleanup temporary manifests
      file:
        path: "/tmp/ajasta-manifests/"
        state: absent
